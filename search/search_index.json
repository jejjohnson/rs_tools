{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>RSTools</code>","text":""},{"location":"installation/","title":"Installation","text":"<p>We can install via the github repo</p> <pre><code>pip install git+https://github.com/space-ml/rs_tools.git\n</code></pre>"},{"location":"installation/#development-version","title":"Development Version","text":"<pre><code>git clone https://github.com/space-ml/rs_tools.git\ncd rs_tools\npoetry install\n</code></pre> <p>Tip</p> <p>We advise you to create a virtual environment before installing:</p> <pre><code>conda env create -f environment.yaml\nconda activate rs_tools\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>poetry run pytest tests/\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>RSTOOLS<ul> <li>Source<ul> <li>Data<ul> <li>GOES<ul> <li>Download</li> <li>Downloader_Goes16</li> </ul> </li> <li>MODIS<ul> <li>Bands</li> <li>Download</li> <li>Downloader_Aqua</li> <li>Downloader_Terra</li> </ul> </li> <li>MSG<ul> <li>Download</li> <li>Downloader_Msg</li> </ul> </li> </ul> </li> <li>Geoprocessing<ul> <li>GOES<ul> <li>Geoprocessor_Goes16</li> <li>Interp</li> <li>Reproject</li> <li>Validation</li> </ul> </li> <li>Grid</li> <li>Interp</li> <li>Match</li> <li>MODIS<ul> <li>Geoprocessor_Modis</li> <li>Interp</li> <li>Reproject</li> <li>Rescale</li> </ul> </li> <li>MSG<ul> <li>Geoprocessor_Msg</li> <li>Reproject</li> </ul> </li> <li>Reproject</li> <li>Utils</li> </ul> </li> <li>Preprocessing<ul> <li>Normalize</li> <li>Prepatcher</li> </ul> </li> <li>Utils<ul> <li>Io</li> <li>Math</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/_src/data/goes/download/","title":"Download","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download","title":"<code>goes_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', daily_window_t0='00:00:00', daily_window_t1='23:59:00', time_step=None, satellite_number=16, save_dir='.', instrument='ABI', processing_level='L1b', data_product='Rad', domain='F', bands='all', check_bands_downloaded=False)</code>","text":"<p>Downloads GOES satellite data for a specified time period and set of bands.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.</p> <code>'00:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.</p> <code>'23:59:00'</code> <code>time_step</code> <code>str</code> <p>The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.</p> <code>None</code> <code>satellite_number</code> <code>int</code> <p>The satellite number. Default is 16.</p> <code>16</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>instrument</code> <code>str</code> <p>The instrument name. Default is 'ABI'.</p> <code>'ABI'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1b'.</p> <code>'L1b'</code> <code>data_product</code> <code>str</code> <p>The data product to download. Default is 'Rad'.</p> <code>'Rad'</code> <code>domain</code> <code>str</code> <p>The domain of the data. Default is 'F' - Full Disk.</p> <code>'F'</code> <code>bands</code> <code>str</code> <p>The bands to download. Default is 'all'.</p> <code>'all'</code> <code>check_bands_downloaded</code> <code>bool</code> <p>Whether to check if all bands were successfully downloaded for each time step. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_1","title":"=========================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--goes-level-1b-test-cases","title":"GOES LEVEL 1B Test Cases","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_2","title":"=========================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day","title":"custom day","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points","title":"custom day + end points","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points-time-window","title":"custom day + end points + time window","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points-time-window-timestep","title":"custom day + end points + time window + timestep","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00 --time-step 06:00:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_3","title":"===================================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--goes-level-2-cloud-mask-test-cases","title":"GOES LEVEL 2 CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_4","title":"===================================","text":"<p>python scripts/goes-download.py 2020-10-01 --start-time 10:00:00 --end-time 11:00:00 --processing-level L2 --data-product ACM</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_5","title":"====================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_6","title":"====================","text":"<p>python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ --check-bands-downloaded</p> Source code in <code>rs_tools/_src/data/goes/download.py</code> <pre><code>def goes_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00',\n    end_time: Optional[str]='23:59:00',\n    daily_window_t0: Optional[str]='00:00:00',\n    daily_window_t1: Optional[str]='23:59:00',\n    time_step: Optional[str]=None,\n    satellite_number: int=16,\n    save_dir: Optional[str] = \".\",\n    instrument: str = \"ABI\",\n    processing_level: str = 'L1b',\n    data_product: str = 'Rad',\n    domain: str = 'F',\n    bands: Optional[str] = \"all\",\n    check_bands_downloaded: bool = False,\n):\n    \"\"\"\n    Downloads GOES satellite data for a specified time period and set of bands.\n\n    Args:\n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        daily_window_t0 (str, optional): The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.\n        daily_window_t1 (str, optional): The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.\n        time_step (str, optional): The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.\n        satellite_number (int, optional): The satellite number. Default is 16.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        instrument (str, optional): The instrument name. Default is 'ABI'.\n        processing_level (str, optional): The processing level of the data. Default is 'L1b'.\n        data_product (str, optional): The data product to download. Default is 'Rad'.\n        domain (str, optional): The domain of the data. Default is 'F' - Full Disk.\n        bands (str, optional): The bands to download. Default is 'all'.\n        check_bands_downloaded (bool, optional): Whether to check if all bands were successfully downloaded for each time step. Default is False.\n\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n        # =========================\n        # GOES LEVEL 1B Test Cases\n        # =========================\n        # custom day\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01\n        # custom day + end points\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00\n        # custom day + end points + time window\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00\n        # custom day + end points + time window + timestep\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00 --time-step 06:00:00\n        # ===================================\n        # GOES LEVEL 2 CLOUD MASK Test Cases\n        # ===================================\n        python scripts/goes-download.py 2020-10-01 --start-time 10:00:00 --end-time 11:00:00 --processing-level L2 --data-product ACM\n\n        # ====================\n        # FAILURE TEST CASES\n        # ====================\n        python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/\n        python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ --check-bands-downloaded\n    \"\"\"\n\n    # run checks\n    # check satellite details\n    _check_input_processing_level(processing_level=processing_level)\n    _check_instrument(instrument=instrument)\n    _check_satellite_number(satellite_number=satellite_number)\n    logger.info(f\"Satellite Number: {satellite_number}\")\n    _check_domain(domain=domain)\n    # compile bands\n    if processing_level == 'L1b':\n        list_of_bands = _check_bands(bands=bands)\n    elif processing_level == 'L2':\n        list_of_bands = None\n    else:\n        raise ValueError('bands not correctly specified for given processing level')\n    # check data product\n    data_product = f\"{instrument}-{processing_level}-{data_product}{domain}\"\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str, end_datetime_str)\n    # datetime conversion \n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # define time step for data query                       \n    if time_step is None: \n        time_step = '1:00:00'\n        logger.info(\"No timedelta specified. Default is 1 hour.\")\n    _check_timedelta_format(time_delta=time_step)\n\n    # convert str to datetime object\n    hours, minutes, seconds = convert_str2time(time=time_step)\n    time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n\n    _check_timedelta(time_delta=time_delta, domain=domain)\n\n    # Compile list of dates/times\n    list_of_dates = np.arange(start_datetime, end_datetime + time_delta, time_delta).astype(datetime)\n    print('Times to check: ',list_of_dates[0], list_of_dates[-1])\n\n    window_date = '1991-10-19' # Add arbitrary date to convert into proper datetime object\n    start_datetime_window_str = window_date + ' ' + str(daily_window_t0)\n    end_datetime_window_str = window_date + ' ' + str(daily_window_t1)\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n    # datetime conversion \n    daily_window_t0_datetime = datetime.strptime(start_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    daily_window_t1_datetime = datetime.strptime(end_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=daily_window_t0_datetime, end_datetime=daily_window_t1_datetime)\n\n    # filter function - check that query times fall within desired time window\n    def is_in_between(date):\n       return daily_window_t0_datetime.time() &lt;= date.time() &lt;= daily_window_t1_datetime.time()\n\n    # compile new list of dates within desired time window\n    list_of_dates = list(filter(is_in_between, list_of_dates))\n    if list_of_dates == []:\n        msg = f\"No times within the specified time window {daily_window_t0_datetime.time()} - {daily_window_t1_datetime.time()} for time step {time_step}\"\n        msg += f\"\\n adjust daily window times or time step\"\n        raise ValueError(msg)\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    files = []\n\n    # create progress bars for dates and bands\n    pbar_time = tqdm.tqdm(list_of_dates)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Time - {itime}\")\n\n        if processing_level == 'L1b':\n            sub_files_list = _goes_level1_download(\n                time=itime, \n                list_of_bands=list_of_bands,\n                satellite_number=satellite_number,\n                data_product=data_product,\n                domain=domain,\n                save_dir=save_dir,\n                check_bands_downloaded=check_bands_downloaded,\n                )\n        elif processing_level == 'L2':\n            sub_files_list = _goes_level2_download(\n                time=itime, \n                satellite_number=satellite_number,\n                data_product=data_product,\n                domain=domain,\n                save_dir=save_dir)\n        else:\n            raise ValueError(f\"Unrecognized processing level: {processing_level}\")\n\n        files += sub_files_list\n\n    return files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/","title":"Downloader Goes16","text":""},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download","title":"<code>GOES16Download</code>  <code>dataclass</code>","text":"<p>A class for downloading GOES 16 data and cloud mask</p> <p>Attributes:</p> Name Type Description <code>start_date</code> <code>str</code> <p>The start date of the data to download.</p> <code>end_date</code> <code>str</code> <p>The end date of the data to download.</p> <code>start_time</code> <code>str</code> <p>The start time of the data to download.</p> <code>end_time</code> <code>str</code> <p>The end time of the data to download.</p> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window for data download.</p> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window for data download.</p> <code>time_step</code> <code>str</code> <p>The time step for data download.</p> <code>save_dir</code> <code>str</code> <p>The directory to save the downloaded data.</p> <p>Methods:</p> Name Description <code>download</code> <p>Downloads GOES 16 data.</p> <code>download_cloud_mask</code> <p>Downloads GOES 16 cloud mask.</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>@dataclass\nclass GOES16Download:\n    \"\"\"A class for downloading GOES 16 data and cloud mask\n\n    Attributes:\n        start_date (str): The start date of the data to download.\n        end_date (str): The end date of the data to download.\n        start_time (str): The start time of the data to download.\n        end_time (str): The end time of the data to download.\n        daily_window_t0 (str): The start time of the daily window for data download.\n        daily_window_t1 (str): The end time of the daily window for data download.\n        time_step (str): The time step for data download.\n        save_dir (str): The directory to save the downloaded data.\n\n    Methods:\n        download: Downloads GOES 16 data.\n        download_cloud_mask: Downloads GOES 16 cloud mask.  \n    \"\"\"\n\n    start_date: str\n    end_date: str \n    start_time: str \n    end_time: str \n    daily_window_t0: str  \n    daily_window_t1: str  \n    time_step: str\n    save_dir: str \n\n    def download(self) -&gt; List[str]:\n        \"\"\"Downloads GOES 16 data\"\"\"\n        goes_files = goes_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite_number=16,\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            instrument=\"ABI\",\n            processing_level='L1b',\n            data_product='Rad',\n            domain='F',\n            bands='all',\n            check_bands_downloaded=True,\n        )\n        return goes_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        \"\"\"Downloads GOES 16 cloud mask\"\"\"\n        goes_files = goes_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite_number=16,\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            instrument=\"ABI\",\n            processing_level='L2',\n            data_product='ACM',\n            domain='F',\n            bands='all',\n            check_bands_downloaded=True,\n        )\n        return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download.download","title":"<code>download()</code>","text":"<p>Downloads GOES 16 data</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download(self) -&gt; List[str]:\n    \"\"\"Downloads GOES 16 data\"\"\"\n    goes_files = goes_download(\n        start_date=self.start_date,\n        end_date=self.end_date,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        daily_window_t0=self.daily_window_t0, \n        daily_window_t1=self.daily_window_t1, \n        time_step=self.time_step,\n        satellite_number=16,\n        save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n        instrument=\"ABI\",\n        processing_level='L1b',\n        data_product='Rad',\n        domain='F',\n        bands='all',\n        check_bands_downloaded=True,\n    )\n    return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download.download_cloud_mask","title":"<code>download_cloud_mask()</code>","text":"<p>Downloads GOES 16 cloud mask</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download_cloud_mask(self) -&gt; List[str]:\n    \"\"\"Downloads GOES 16 cloud mask\"\"\"\n    goes_files = goes_download(\n        start_date=self.start_date,\n        end_date=self.end_date,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        daily_window_t0=self.daily_window_t0, \n        daily_window_t1=self.daily_window_t1, \n        time_step=self.time_step,\n        satellite_number=16,\n        save_dir=Path(self.save_dir).joinpath(\"CM\"),\n        instrument=\"ABI\",\n        processing_level='L2',\n        data_product='ACM',\n        domain='F',\n        bands='all',\n        check_bands_downloaded=True,\n    )\n    return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.download","title":"<code>download(start_date='2020-10-02', end_date='2020-10-02', start_time='14:00:00', end_time='20:00:00', daily_window_t0='14:00:00', daily_window_t1='14:30:00', time_step='00:15:00', save_dir='./data/', cloud_mask=True)</code>","text":"<p>Downloads GOES 16 data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>end_date</code> <code>str</code> <p>The end date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>start_time</code> <code>str</code> <p>The start time of the data to download (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data to download (format: 'HH:MM:SS')</p> <code>'20:00:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window (format: 'HH:MM:SS')</p> <code>'14:30:00'</code> <code>time_step</code> <code>str</code> <p>The time step between consecutive data downloads (format: 'HH:MM:SS')</p> <code>'00:15:00'</code> <code>save_path</code> <code>str</code> <p>The path to save the downloaded data</p> required <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <p>List[str]: List of downloaded file names</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download(\n        start_date: str=\"2020-10-02\",\n        end_date: str=\"2020-10-02\", \n        start_time: str=\"14:00:00\", \n        end_time: str=\"20:00:00\", \n        daily_window_t0: str=\"14:00:00\",  \n        daily_window_t1: str=\"14:30:00\",  \n        time_step: str=\"00:15:00\",\n        save_dir: str='./data/', \n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads GOES 16 data including cloud mask\n\n    Args:\n        start_date (str): The start date of the data to download (format: 'YYYY-MM-DD')\n        end_date (str): The end date of the data to download (format: 'YYYY-MM-DD')\n        start_time (str): The start time of the data to download (format: 'HH:MM:SS')\n        end_time (str): The end time of the data to download (format: 'HH:MM:SS')\n        daily_window_t0 (str): The start time of the daily window (format: 'HH:MM:SS')\n        daily_window_t1 (str): The end time of the daily window (format: 'HH:MM:SS')\n        time_step (str): The time step between consecutive data downloads (format: 'HH:MM:SS')\n        save_path (str): The path to save the downloaded data\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True)\n\n    Returns:\n        List[str]: List of downloaded file names\n    \"\"\"\n    # Initialize GOES 16 Downloader\n    logger.info(\"Initializing GOES16 Downloader...\")\n    dc_goes16_download = GOES16Download(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        daily_window_t0=daily_window_t0,\n        daily_window_t1=daily_window_t1,\n        time_step=time_step,\n        save_dir=Path(save_dir).joinpath(\"goes16\"),\n    )\n    logger.info(\"Downloading GOES 16 Data...\")\n    goes16_filenames = dc_goes16_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading GOES 16 Cloud Mask...\")\n        goes16_filenames = dc_goes16_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished GOES 16 Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/modis/bands/","title":"Bands","text":""},{"location":"api/_src/data/modis/download/","title":"Download","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.get_daily_window","title":"<code>get_daily_window(daily_start, end_time)</code>","text":"<p>computes tuple of start and end date/time for each day for earthaccess call</p> Source code in <code>rs_tools/_src/data/modis/download.py</code> <pre><code>def get_daily_window(daily_start, end_time):\n    \"\"\"computes tuple of start and end date/time for each day for earthaccess call\"\"\"\n    day = daily_start.strftime(\"%Y-%m-%d\")\n    daily_end = day + ' ' + end_time\n    return (daily_start.strftime(\"%Y-%m-%d %H:%M:%S\"), daily_end)\n</code></pre>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download","title":"<code>modis_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', day_step=1, satellite='Terra', save_dir='.', processing_level='L1b', resolution='1KM', bounding_box=(-180, -90, 180, 90), earthdata_username='', earthdata_password='', day_night_flag=None, identifier='02')</code>","text":"<p>Downloads MODIS satellite data for a specified time period and location.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>day_step</code> <code>int</code> <p>The time step (in days) between downloads. This is to allow the user to download data every e.g. 2 days. If not provided, the default is daily downloads.</p> <code>1</code> <code>satellite</code> <code>str</code> <p>The satellite. Options are \"Terra\" and \"Aqua\", with \"Terra\" as default.</p> <code>'Terra'</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1b'.</p> <code>'L1b'</code> <code>resolution</code> <code>str</code> <p>The resolution of the data. Options are \"QKM\" (250m), \"HKM (500m), \"1KM\" (1000m), with \"1KM\" as default. Not all bands are measured at all resolutions.</p> <code>'1KM'</code> <code>bounding_box</code> <code>tuple</code> <p>The region to be downloaded.</p> <code>(-180, -90, 180, 90)</code> <code>earthdata_username</code> <code>str</code> <p>Username associated with the NASA Earth Data login. Required for download.</p> <code>''</code> <code>earthdata_password</code> <code>str</code> <p>Password associated with the NASA Earth Data login. Required for download.</p> <code>''</code> <code>day_night_flag</code> <code>str</code> <p>The time of day for the data. Options are \"day\" and \"night\". If not provided, both day and night data will be downloaded.</p> <code>None</code> <code>identifier</code> <code>str</code> <p>The MODIS data product identifier. Options are \"02\" and \"35\". Default is \"02\".</p> <code>'02'</code> <p>Returns:     list: A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_1","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--modis-level-1b-test-cases","title":"MODIS LEVEL 1B Test Cases","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_2","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--one-day-successfully-downloaded-4-granules-all-nighttime","title":"one day - successfully downloaded 4 granules (all nighttime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/test_script/</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--multiple-days-finds-62-granules-stopped-download-for-times-sake-but-seemed-to-work","title":"multiple days - finds 62 granules, stopped download for times sake but seemed to work","text":"<p>python scripts/modis-download.py 2018-10-01 --end-date 2018-10-9 --day-step 3 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--test-bounding-box-successfully-downloaded-4-files-all-daytime","title":"test bounding box - successfully downloaded 4 files (all daytime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 20 5</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--test-daynight-flag-successfully-downloaded-1-file-daytime-only","title":"test day/night flag - successfully downloaded 1 file (daytime only)","text":"<p>python scripts/modis-download.py 2018-10-15 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 10 -5 15 --day-night-flag day</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_3","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--modis-level-2-cloud-mask-test-cases","title":"MODIS LEVEL 2 CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_4","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--one-day-successfully-downloaded-4-granules-all-nighttime_1","title":"one day - successfully downloaded 4 granules (all nighttime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/ --processing-level L2 --identifier 35</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_5","title":"====================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_6","title":"====================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--bounding-box-input-invalid-throws-error-as-expected","title":"bounding box input invalid - throws error as expected","text":"<p>python scripts/modis-download.py 2018-10-01 --bounding-box a b c d</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--end-date-before-start-date-throws-error-as-expected","title":"end date before start date - throws error as expected","text":"<p>python scripts/modis-download.py 2018-10-01  --end-date 2018-09-01 </p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--empty-results-warns-user-as-expected","title":"empty results - warns user as expected","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 07:00:00 --end-time 7:10:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 -5 -5</p> Source code in <code>rs_tools/_src/data/modis/download.py</code> <pre><code>def modis_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00', # used for daily window\n    end_time: Optional[str]='23:59:00', # used for daily window\n    day_step: Optional[int]=1, \n    satellite: str='Terra',\n    save_dir: Optional[str]=\".\",\n    processing_level: str = 'L1b',\n    resolution: str = \"1KM\",\n    bounding_box: Optional[tuple[float, float, float, float]]=(-180, -90, 180, 90), # TODO: Add polygon option\n    earthdata_username: Optional[str]=\"\",\n    earthdata_password: Optional[str]=\"\",\n    day_night_flag: Optional[str]=None, \n    identifier: Optional[str] = \"02\"\n):\n    \"\"\"\n    Downloads MODIS satellite data for a specified time period and location.\n\n    Args:        \n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        day_step (int, optional): The time step (in days) between downloads. This is to allow the user to download data every e.g. 2 days. If not provided, the default is daily downloads.\n        satellite (str, optional): The satellite. Options are \"Terra\" and \"Aqua\", with \"Terra\" as default.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        processing_level (str, optional): The processing level of the data. Default is 'L1b'.\n        resolution (str, optional): The resolution of the data. Options are \"QKM\" (250m), \"HKM (500m), \"1KM\" (1000m), with \"1KM\" as default. Not all bands are measured at all resolutions.\n        bounding_box (tuple, optional): The region to be downloaded.\n        earthdata_username (str): Username associated with the NASA Earth Data login. Required for download.\n        earthdata_password (str): Password associated with the NASA Earth Data login. Required for download.\n        day_night_flag (str, optional): The time of day for the data. Options are \"day\" and \"night\". If not provided, both day and night data will be downloaded.\n        identifier (str, optional): The MODIS data product identifier. Options are \"02\" and \"35\". Default is \"02\".\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n    # =========================\n    # MODIS LEVEL 1B Test Cases\n    # =========================\n    # one day - successfully downloaded 4 granules (all nighttime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/test_script/\n\n    # multiple days - finds 62 granules, stopped download for times sake but seemed to work\n    python scripts/modis-download.py 2018-10-01 --end-date 2018-10-9 --day-step 3 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/\n\n    # test bounding box - successfully downloaded 4 files (all daytime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 20 5\n\n    # test day/night flag - successfully downloaded 1 file (daytime only)\n    python scripts/modis-download.py 2018-10-15 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 10 -5 15 --day-night-flag day\n\n    # =========================\n    # MODIS LEVEL 2 CLOUD MASK Test Cases\n    # =========================\n\n    # one day - successfully downloaded 4 granules (all nighttime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/ --processing-level L2 --identifier 35\n\n    # ====================\n    # FAILURE TEST CASES\n    # ====================\n    # bounding box input invalid - throws error as expected\n    python scripts/modis-download.py 2018-10-01 --bounding-box a b c d\n\n    # end date before start date - throws error as expected\n    python scripts/modis-download.py 2018-10-01  --end-date 2018-09-01 \n\n    # empty results - warns user as expected\n    python scripts/modis-download.py 2018-10-01 --start-time 07:00:00 --end-time 7:10:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 -5 -5\n\n    \"\"\"\n    # check if earthdata login is available\n    _check_earthdata_login(earthdata_username=earthdata_username, earthdata_password=earthdata_password)\n\n    # check if netcdf4 backend is available\n    _check_netcdf4_backend()\n\n    # run checks\n    # translate str inputs to modis specific names\n    _check_input_processing_level(processing_level=processing_level)\n    _check_identifier(identifier=identifier)\n    satellite_code = _check_satellite(satellite=satellite)\n    resolution_code = _check_resolution(resolution=resolution)\n    logger.info(f\"Satellite: {satellite}\")\n    # check data product\n    if processing_level == 'L1b':\n        data_product = f\"{satellite_code}{identifier}{resolution_code}\"\n    elif processing_level == 'L2':\n        # TODO: Implement other level-2 products or allow passing in data_product?\n        # NOTE: Resolution argument not needed for cloud mask download\n        data_product = f\"{satellite_code}{identifier}_{processing_level}\"\n    else:\n        raise ValueError(\"Incorrect processing level, downloader only implemented for 'L1b' and 'L2'\")\n\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str=start_datetime_str, end_datetime_str=end_datetime_str) \n    # datetime conversion\n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_dates(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # compile list of dates/times \n    day_delta = timedelta(days=day_step)\n    list_of_dates = np.arange(start_datetime, end_datetime, day_delta).astype(datetime)\n\n    list_of_daily_windows = [get_daily_window(daily_start, end_time) for daily_start in list_of_dates]\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    # check that bounding box is valid\n    # TODO: Add option to add multiple location requests\n    # NOTE: earthaccess allows other ways to specify spatial extent, e.g. polygon, point \n    # NOTE: extend to allow these options\n    _check_bounding_box(bounding_box=bounding_box)\n\n    # create dictionary of earthaccess search parameters\n    search_params = {\n        \"short_name\": data_product,\n        \"bounding_box\": bounding_box,\n    }\n\n    # if day_night_flag was provided, check that day_night_flag is valid\n    if day_night_flag: \n        _check_day_night_flag(day_night_flag=day_night_flag)\n        # add day_night_flag to search parameters\n        search_params[\"day_night_flag\"] = day_night_flag\n\n    # TODO: remove - logging search_params for testing\n    logger.info(f\"Search parameters: {search_params}\")\n\n    files = []\n\n    # create progress bar for dates\n    pbar_time = tqdm.tqdm(list_of_daily_windows)\n\n    for itime in pbar_time:\n        pbar_time.set_description(f\"Time - {itime[0]} to {itime[1]}\")\n        success_flag = True\n\n        # add daytime window to search parameters\n        search_params[\"temporal\"] = itime\n\n        # search for data\n        results_day = earthaccess.search_data(**search_params)\n\n        # check if any results were returned\n        if not results_day:\n            # if not: log warning and continue to next date\n            success_flag = False\n            warn = f\"No data found for {itime[0]} to {itime[1]} in the specified bounding box\"\n            if day_night_flag: warn += f\" for {day_night_flag}-time measurements only\"\n            logger.warning(warn)\n            continue\n\n        files_day = earthaccess.download(results_day, save_dir) \n        # TODO: can this fail? if yes, use try / except to prevent the programme from crashing\n        # TODO: check file sizes - if less than X MB (ca 70MB) the download failed\n        if success_flag:\n            files += files_day\n\n    return files    \n</code></pre>"},{"location":"api/_src/data/modis/downloader_aqua/","title":"Downloader Aqua","text":"<p>A General Pipeline for create ML-Ready Data - Downloading the Data - Data Harmonization - Normalizing - Patching</p>"},{"location":"api/_src/data/modis/downloader_aqua/#rs_tools._src.data.modis.downloader_aqua.MODISAquaDownload","title":"<code>MODISAquaDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for AQUA/MODIS data and cloud mask</p> Source code in <code>rs_tools/_src/data/modis/downloader_aqua.py</code> <pre><code>@dataclass\nclass MODISAquaDownload:\n    \"\"\"Downloading class for AQUA/MODIS data and cloud mask\"\"\"\n    start_date: str\n    end_date: str\n    start_time: str\n    end_time: str\n    save_dir: str\n    bounding_box: tuple[float, float, float, float]\n\n    def download(self) -&gt; List[str]:\n        aqua_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Aqua\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            processing_level='L1b',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"02\"\n            )\n        return aqua_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        aqua_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Aqua\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            processing_level='L2',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"35\"\n            )\n        return aqua_files\n</code></pre>"},{"location":"api/_src/data/modis/downloader_aqua/#rs_tools._src.data.modis.downloader_aqua.download","title":"<code>download(start_date='2020-10-01', end_date='2020-10-01', start_time='14:00:00', end_time='21:00:00', save_dir='./data/', region='-130 -15 -90 5', cloud_mask=True)</code>","text":"<p>Downloads AQUA MODIS data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>end_date</code> <code>str</code> <p>The end date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>start_time</code> <code>str</code> <p>The start time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'21:00:00'</code> <code>save_dir</code> <code>str</code> <p>The directory path to save the downloaded files.</p> <code>'./data/'</code> <code>region</code> <code>str</code> <p>The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".</p> <code>'-130 -15 -90 5'</code> <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/data/modis/downloader_aqua.py</code> <pre><code>def download(\n        start_date: str = \"2020-10-01\", \n        end_date: str = \"2020-10-01\",\n        start_time: str = \"14:00:00\",\n        end_time: str = \"21:00:00\",\n        save_dir: str = \"./data/\",\n        region: str = \"-130 -15 -90 5\",\n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads AQUA MODIS data including cloud mask\n\n    Args:\n        start_date (str): The start date of the period to download files for in the format \"YYYY-MM-DD\".\n        end_date (str): The end date of the period to download files for in the format \"YYYY-MM-DD\".\n        start_time (str): The start time of the period to download files for in the format \"HH:MM:SS\".\n        end_time (str): The end time of the period to download files for in the format \"HH:MM:SS\".\n        save_dir (str): The directory path to save the downloaded files.\n        region (str, optional): The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True).\n\n    Returns:\n        None\n    \"\"\"\n    bounding_box = tuple(map(lambda x: int(x), region.split(\" \")))\n    # Initialize AQUA Downloader\n    logger.info(\"Initializing AQUA Downloader...\")\n    dc_aqua_download = MODISAquaDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        save_dir=Path(save_dir).joinpath(\"aqua\"),\n        bounding_box=bounding_box,\n    )\n    logger.info(\"Downloading AQUA Data...\")\n    modis_filenames = dc_aqua_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading AQUA Cloud Mask...\")\n        modis_filenames = dc_aqua_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished AQUA Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/modis/downloader_terra/","title":"Downloader Terra","text":"<p>A General Pipeline for create ML-Ready Data - Downloading the Data - Data Harmonization - Normalizing - Patching</p>"},{"location":"api/_src/data/modis/downloader_terra/#rs_tools._src.data.modis.downloader_terra.MODISTerraDownload","title":"<code>MODISTerraDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for TERRA/MODIS data and cloud mask</p> Source code in <code>rs_tools/_src/data/modis/downloader_terra.py</code> <pre><code>@dataclass\nclass MODISTerraDownload:\n    \"\"\"Downloading class for TERRA/MODIS data and cloud mask\"\"\"\n    start_date: str\n    end_date: str\n    start_time: str\n    end_time: str\n    save_dir: str\n    bounding_box: tuple[float, float, float, float]\n\n    def download(self) -&gt; List[str]:\n        terra_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Terra\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            processing_level='L1b',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"02\"\n            )\n        return terra_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        terra_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Terra\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            processing_level='L2',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"35\"\n            )\n        return terra_files\n</code></pre>"},{"location":"api/_src/data/modis/downloader_terra/#rs_tools._src.data.modis.downloader_terra.download","title":"<code>download(start_date='2020-10-01', end_date='2020-10-01', start_time='14:00:00', end_time='21:00:00', save_dir='./data/', region='-130 -15 -90 5', cloud_mask=True)</code>","text":"<p>Downloads TERRA MODIS data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>end_date</code> <code>str</code> <p>The end date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>start_time</code> <code>str</code> <p>The start time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'21:00:00'</code> <code>save_dir</code> <code>str</code> <p>The directory path to save the downloaded files.</p> <code>'./data/'</code> <code>region</code> <code>str</code> <p>The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".</p> <code>'-130 -15 -90 5'</code> <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/data/modis/downloader_terra.py</code> <pre><code>def download(\n        start_date: str = \"2020-10-01\", \n        end_date: str = \"2020-10-01\",\n        start_time: str = \"14:00:00\",\n        end_time: str = \"21:00:00\",\n        save_dir: str = \"./data/\",\n        region: str = \"-130 -15 -90 5\",\n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads TERRA MODIS data including cloud mask\n\n    Args:\n        start_date (str): The start date of the period to download files for in the format \"YYYY-MM-DD\".\n        end_date (str): The end date of the period to download files for in the format \"YYYY-MM-DD\".\n        start_time (str): The start time of the period to download files for in the format \"HH:MM:SS\".\n        end_time (str): The end time of the period to download files for in the format \"HH:MM:SS\".\n        save_dir (str): The directory path to save the downloaded files.\n        region (str, optional): The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True).\n\n    Returns:\n        None\n    \"\"\"\n    bounding_box = tuple(map(lambda x: int(x), region.split(\" \")))\n    # Initialize TERRA Downloader\n    logger.info(\"Initializing TERRA Downloader...\")\n    dc_terra_download = MODISTerraDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        save_dir=Path(save_dir).joinpath(\"terra\"),\n        bounding_box=bounding_box,\n    )\n    logger.info(\"Downloading TERRA Data...\")\n    modis_filenames = dc_terra_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading TERRA Cloud Mask...\")\n        modis_filenames = dc_terra_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished TERRA Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/msg/download/","title":"Download","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download","title":"<code>msg_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', daily_window_t0='00:00:00', daily_window_t1='23:59:00', time_step=None, satellite='MSG', instrument='HRSEVIRI', processing_level='L1', save_dir='.', eumdac_key='', eumdac_secret='')</code>","text":"<p>Downloads GOES satellite data for a specified time period and set of bands.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.</p> <code>'00:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.</p> <code>'23:59:00'</code> <code>time_step</code> <code>str</code> <p>The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.</p> <code>None</code> <code>satellite</code> <code>str</code> <p>The satellite. Default is MSG.</p> <code>'MSG'</code> <code>instrument</code> <code>str</code> <p>The data product to download. Default is 'HRSEVIRI', also implemented for Cloud Mask (CLM).</p> <code>'HRSEVIRI'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1'.</p> <code>'L1'</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>eumdac_key</code> <code>str</code> <p>The EUMETSAT Data Centre (EUMDAC) API key. If not provided, the user will be prompted to enter the key.</p> <code>''</code> <code>eumdac_secret</code> <code>str</code> <p>The EUMETSAT Data Centre (EUMDAC) API secret. If not provided, the user will be prompted to enter the secret.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_1","title":"=========================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--msg-level-1-test-cases","title":"MSG LEVEL 1 Test Cases","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_2","title":"=========================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day","title":"custom day","text":"<p>python scripts/msg-download.py 2018-10-01 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points","title":"custom day + end points","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 09:00:00 --end-time 12:00:00 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:05:00 --end-time 12:05:00</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window","title":"custom day + end points + time window","text":"<p>scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window-timestep","title":"custom day + end points + time window + timestep","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:15:00 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:25:00</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_3","title":"===================================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--msg-cloud-mask-test-cases","title":"MSG CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_4","title":"===================================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day_1","title":"custom day","text":"<p>python scripts/msg-download.py 2018-10-01 --instrument=CLM</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points_1","title":"custom day + end points","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --instrument=CLM </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window_1","title":"custom day + end points + time window","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --instrument=CLM </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window-timestep_1","title":"custom day + end points + time window + timestep","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --time-step 00:25:00 --instrument=CLM</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_5","title":"====================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_6","title":"====================","text":"Source code in <code>rs_tools/_src/data/msg/download.py</code> <pre><code>def msg_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00', # EUMDAC did not find any data for 00:00:00\n    end_time: Optional[str]='23:59:00', # EUMDAC did not find any data for 23:59:00\n    daily_window_t0: Optional[str]='00:00:00',\n    daily_window_t1: Optional[str]='23:59:00',\n    time_step: Optional[str]=None,\n    satellite: str=\"MSG\",\n    instrument: str =\"HRSEVIRI\",\n    processing_level: Optional[str] = \"L1\",\n    save_dir: Optional[str] = \".\",\n    eumdac_key: Optional[str]=\"\",\n    eumdac_secret: Optional[str]=\"\",\n):\n    \"\"\"\n    Downloads GOES satellite data for a specified time period and set of bands.\n\n    Args:\n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        daily_window_t0 (str, optional): The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.\n        daily_window_t1 (str, optional): The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.\n        time_step (str, optional): The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.\n        satellite (str): The satellite. Default is MSG.\n        instrument (str): The data product to download. Default is 'HRSEVIRI', also implemented for Cloud Mask (CLM).\n        processing_level (str, optional): The processing level of the data. Default is 'L1'.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        eumdac_key (str, optional): The EUMETSAT Data Centre (EUMDAC) API key. If not provided, the user will be prompted to enter the key.\n        eumdac_secret (str, optional): The EUMETSAT Data Centre (EUMDAC) API secret. If not provided, the user will be prompted to enter the secret.\n\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n        # =========================\n        # MSG LEVEL 1 Test Cases\n        # =========================\n        # custom day\n        python scripts/msg-download.py 2018-10-01\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01\n        # custom day + end points\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 09:00:00 --end-time 12:00:00\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:05:00 --end-time 12:05:00\n        # custom day + end points + time window\n        scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 \n        # custom day + end points + time window + timestep\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:15:00\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:25:00\n        # ===================================\n        # MSG CLOUD MASK Test Cases\n        # ===================================\n        # custom day\n        python scripts/msg-download.py 2018-10-01 --instrument=CLM\n        # custom day + end points\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --instrument=CLM \n        # custom day + end points + time window\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --instrument=CLM \n        # custom day + end points + time window + timestep\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --time-step 00:25:00 --instrument=CLM\n        # ====================\n        # FAILURE TEST CASES\n        # ====================\n    \"\"\"\n\n    # run checks\n    # check if eumdac login is available\n    token = _check_eumdac_login(eumdac_key=eumdac_key, eumdac_secret=eumdac_secret)\n    datastore = eumdac.DataStore(token)\n\n    # check if netcdf4 backend is available\n    _check_netcdf4_backend()\n\n    # check satellite details\n    _check_input_processing_level(processing_level=processing_level)\n    _check_instrument(instrument=instrument)\n    # check data product\n    data_product = f\"EO:EUM:DAT:{satellite}:{instrument}\"\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str, end_datetime_str)\n    # datetime conversion \n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # define time step for data query                       \n    if time_step is None: \n        time_step = '1:00:00'\n        logger.info(\"No timedelta specified. Default is 1 hour.\")\n    _check_timedelta_format(time_delta=time_step)\n\n    # convert str to datetime object\n    hours, minutes, seconds = convert_str2time(time=time_step)\n    time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n\n    _check_timedelta(time_delta=time_delta)\n\n    # Compile list of dates/times\n    list_of_dates = np.arange(start_datetime, end_datetime, time_delta).astype(datetime)\n    print('Times to check: ',list_of_dates[0], list_of_dates[-1])\n\n    window_date = '1991-10-19' # Add arbitrary date to convert into proper datetime object\n    start_datetime_window_str = window_date + ' ' + daily_window_t0\n    end_datetime_window_str = window_date + ' ' + daily_window_t1\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n    # datetime conversion \n    daily_window_t0_datetime = datetime.strptime(start_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    daily_window_t1_datetime = datetime.strptime(end_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=daily_window_t0_datetime, end_datetime=daily_window_t1_datetime)\n\n    # filter function - check that query times fall within desired time window\n    def is_in_between(date):\n       return daily_window_t0_datetime.time() &lt;= date.time() &lt;= daily_window_t1_datetime.time()\n\n    # compile new list of dates within desired time window\n    list_of_dates = list(filter(is_in_between, list_of_dates))\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    files = []\n\n    # create progress bars for dates and bands\n    pbar_time = tqdm.tqdm(list_of_dates)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Time - {itime}\")\n\n        sub_files_list = _download(time=itime, data_product=data_product, save_dir=save_dir, datastore=datastore)\n        if sub_files_list is None:\n            if itime != list_of_dates[-1]:\n                logger.info(f\"Could not find data for time {itime}. Trying to add 5 mins to timestamp.\")\n                time_delta = timedelta(hours=0, minutes=5, seconds=0)\n                itime_5 = itime+ time_delta\n                sub_files_list = _download(time=itime_5, data_product=data_product, save_dir=save_dir, datastore=datastore)\n        if sub_files_list is None:\n            if itime != list_of_dates[-1]:\n                logger.info(f\"Could not find data for time {itime}. Trying to add 10 mins to timestamp.\")\n                time_delta = timedelta(hours=0, minutes=10, seconds=0)\n                itime_10 = itime+ time_delta\n                sub_files_list = _download(time=itime_10, data_product=data_product, save_dir=save_dir, datastore=datastore)\n\n        if sub_files_list is None:\n            logger.info(f\"Could not find data for time {itime}. Skipping to next time.\")\n        else:\n            files += sub_files_list\n\n    return files\n</code></pre>"},{"location":"api/_src/data/msg/downloader_msg/","title":"Downloader Msg","text":""},{"location":"api/_src/data/msg/downloader_msg/#rs_tools._src.data.msg.downloader_msg.MSGDownload","title":"<code>MSGDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for MSG data and cloud mask</p> Source code in <code>rs_tools/_src/data/msg/downloader_msg.py</code> <pre><code>@dataclass\nclass MSGDownload:\n    \"\"\"Downloading class for MSG data and cloud mask\"\"\"\n    start_date: str\n    end_date: str \n    start_time: str \n    end_time: str \n    daily_window_t0: str  \n    daily_window_t1: str  \n    time_step: str\n    save_dir: str \n\n    def download(self) -&gt; List[str]:\n        msg_files = msg_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite=\"MSG\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            instrument=\"HRSEVIRI\",\n            processing_level='L1',\n        )\n        return msg_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        msg_files = msg_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite=\"MSG\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            instrument=\"CLM\",\n            processing_level='L1',\n        )\n        return msg_files\n</code></pre>"},{"location":"api/_src/data/msg/downloader_msg/#rs_tools._src.data.msg.downloader_msg.download","title":"<code>download(start_date='2020-10-02', end_date='2020-10-02', start_time='14:00:00', end_time='20:00:00', daily_window_t0='14:00:00', daily_window_t1='14:30:00', time_step='00:15:00', save_dir='./data/', cloud_mask=True)</code>","text":"<p>Downloads MSG data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>end_date</code> <code>str</code> <p>The end date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>start_time</code> <code>str</code> <p>The start time of the data to download (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data to download (format: 'HH:MM:SS')</p> <code>'20:00:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window (format: 'HH:MM:SS')</p> <code>'14:30:00'</code> <code>time_step</code> <code>str</code> <p>The time step between consecutive data downloads (format: 'HH:MM:SS')</p> <code>'00:15:00'</code> <code>save_path</code> <code>str</code> <p>The path to save the downloaded data</p> required <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <p>List[str]: List of downloaded file names</p> Source code in <code>rs_tools/_src/data/msg/downloader_msg.py</code> <pre><code>def download(\n        start_date: str=\"2020-10-02\",\n        end_date: str=\"2020-10-02\", \n        start_time: str=\"14:00:00\", \n        end_time: str=\"20:00:00\", \n        daily_window_t0: str=\"14:00:00\",  \n        daily_window_t1: str=\"14:30:00\",  \n        time_step: str=\"00:15:00\",\n        save_dir: str='./data/', \n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads MSG data including cloud mask\n\n    Args:\n        start_date (str): The start date of the data to download (format: 'YYYY-MM-DD')\n        end_date (str): The end date of the data to download (format: 'YYYY-MM-DD')\n        start_time (str): The start time of the data to download (format: 'HH:MM:SS')\n        end_time (str): The end time of the data to download (format: 'HH:MM:SS')\n        daily_window_t0 (str): The start time of the daily window (format: 'HH:MM:SS')\n        daily_window_t1 (str): The end time of the daily window (format: 'HH:MM:SS')\n        time_step (str): The time step between consecutive data downloads (format: 'HH:MM:SS')\n        save_path (str): The path to save the downloaded data\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True)\n\n    Returns:\n        List[str]: List of downloaded file names\n    \"\"\"\n    # Initialize MSG Downloader\n    logger.info(\"Initializing MSG Downloader...\")\n    dc_msg_download = MSGDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        daily_window_t0=daily_window_t0,\n        daily_window_t1=daily_window_t1,\n        time_step=time_step,\n        save_dir=Path(save_dir).joinpath(\"msg\"),\n    )\n    logger.info(\"Downloading MSG Data...\")\n    msg_filenames = dc_msg_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading MSG Cloud Mask...\")\n        msg_filenames = dc_msg_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished MSG Downloading Script...\")\n</code></pre>"},{"location":"api/_src/geoprocessing/grid/","title":"Grid","text":""},{"location":"api/_src/geoprocessing/grid/#rs_tools._src.geoprocessing.grid.create_latlon_grid","title":"<code>create_latlon_grid(region, resolution)</code>","text":"<p>Create a latitude-longitude grid within the specified region.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>Tuple[float, float, float, float]</code> <p>The region of interest defined by (lon_min, lat_min, lon_max, lat_max), e.g. (-180, -90, 180, 90) for global grid.</p> required <code>resolution</code> <code>float</code> <p>The resolution of the grid in degrees.</p> required <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays representing the latitudes and longitudes of the grid, respectively.</p> Source code in <code>rs_tools/_src/geoprocessing/grid.py</code> <pre><code>def create_latlon_grid(region: Tuple[float, float, float, float],\n                       resolution: float):\n    \"\"\"\n    Create a latitude-longitude grid within the specified region.\n\n    Args:\n        region (Tuple[float, float, float, float]): The region of interest defined by\n            (lon_min, lat_min, lon_max, lat_max), e.g. (-180, -90, 180, 90) for global grid.\n        resolution (float): The resolution of the grid in degrees.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays representing\n            the latitudes and longitudes of the grid, respectively.\n    \"\"\"\n\n    lat_bnds = region[1], region[3]\n    lon_bnds = region[0], region[2]\n    latitudes = np.arange(lat_bnds[0], lat_bnds[1]+resolution, resolution)\n    longitudes = np.arange(lon_bnds[0], lon_bnds[1]+resolution, resolution)\n    return np.meshgrid(latitudes, longitudes)\n</code></pre>"},{"location":"api/_src/geoprocessing/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/interp/#rs_tools._src.geoprocessing.interp.resample_rioxarray","title":"<code>resample_rioxarray(ds, resolution=(1000, 1000), method='bilinear')</code>","text":"<p>Resamples a raster dataset using rasterio-xarray.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be resampled.</p> required <code>resolution</code> <code>int</code> <p>The desired resolution of the resampled dataset. Default is 1_000.</p> <code>(1000, 1000)</code> <code>method</code> <code>str</code> <p>The resampling method to be used. Default is \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The resampled dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/interp.py</code> <pre><code>def resample_rioxarray(ds: xr.Dataset, resolution: Tuple[int, int]=(1_000, 1_000), method: str=\"bilinear\") -&gt; xr.Dataset:\n    \"\"\"\n    Resamples a raster dataset using rasterio-xarray.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be resampled.\n        resolution (int): The desired resolution of the resampled dataset. Default is 1_000.\n        method (str): The resampling method to be used. Default is \"bilinear\".\n\n    Returns:\n        xr.Dataset: The resampled dataset.\n    \"\"\"\n\n    ds = ds.rio.reproject(\n        ds.rio.crs,\n        resolution=resolution,\n        resample=rioxarray_samplers[method], \n    )\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/match/","title":"Match","text":""},{"location":"api/_src/geoprocessing/match/#rs_tools._src.geoprocessing.match.match_timestamps","title":"<code>match_timestamps(times_data, times_clouds, cutoff=15)</code>","text":"<p>Matches timestamps of data and cloudmask files, if not measured at exactly the same time.</p> <p>Parameters:</p> Name Type Description Default <code>times_data</code> <code>List[str]</code> <p>Timestamps of data files.</p> required <code>times_clouds</code> <code>List[str]</code> <p>Timestamps of the cloud mask files.</p> required <code>cutoff</code> <code>str</code> <p>Maximum time difference in minutes to consider a match. Defaults to 15.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the matched timestamps.</p> Source code in <code>rs_tools/_src/geoprocessing/match.py</code> <pre><code>def match_timestamps(times_data: List[str], times_clouds: List[str], cutoff: int=15) -&gt; pd.DataFrame:\n    \"\"\"\n    Matches timestamps of data and cloudmask files, if not measured at exactly the same time.\n\n    Args:\n        times_data (List[str]): Timestamps of data files.\n        times_clouds (List[str]): Timestamps of the cloud mask files.\n        cutoff (str, optional): Maximum time difference in minutes to consider a match. Defaults to 15.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the matched timestamps.\n    \"\"\"\n    # Convert timestamps to datetime objects\n    timestamps_data = pd.to_datetime(times_data)\n    timestamps_clouds = pd.to_datetime(times_clouds)\n\n    matches_data = []\n    matches_clouds = []\n\n    # Loop through timestamps of data files\n    for time in timestamps_data:\n        # Find closest timestamp in cloud mask files\n        closest_time = timestamps_clouds[\n            np.abs((timestamps_clouds - time).total_seconds()).argmin()\n        ]\n        # Check if the closest timestamp is within the cutoff time\n        if np.abs((closest_time - time).total_seconds()) &lt;= pd.Timedelta(f'{cutoff}min').total_seconds():\n            matches_data.append(time.strftime(\"%Y%m%d%H%M%S\"))\n            matches_clouds.append(closest_time.strftime(\"%Y%m%d%H%M%S\"))\n        else:\n            logger.info(f\"No matching cloud mask found for {time}\")\n\n    matched_times = pd.DataFrame({\n        \"timestamps_data\": matches_data,\n        \"timestamps_cloudmask\": matches_clouds\n    })\n\n    return matched_times\n</code></pre>"},{"location":"api/_src/geoprocessing/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/reproject/#rs_tools._src.geoprocessing.reproject.calc_latlon","title":"<code>calc_latlon(ds)</code>","text":"<p>Calculate the latitude and longitude coordinates for the given dataset</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Xarray Dataset to calculate the lat/lon coordinates for, with x and y coordinates</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Xarray Dataset with the latitude and longitude coordinates added</p> Source code in <code>rs_tools/_src/geoprocessing/reproject.py</code> <pre><code>def calc_latlon(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate the latitude and longitude coordinates for the given dataset\n\n    Args:\n        ds: Xarray Dataset to calculate the lat/lon coordinates for, with x and y coordinates\n\n    Returns:\n        Xarray Dataset with the latitude and longitude coordinates added\n    \"\"\"\n    XX, YY = np.meshgrid(ds.x.data, ds.y.data)\n    lons, lats = convert_x_y_to_lat_lon(ds.rio.crs, XX, YY)\n    # Check if lons and lons_trans are close in value\n    # Set inf to NaN values\n    lons[lons == np.inf] = np.nan\n    lats[lats == np.inf] = np.nan\n\n    ds = ds.assign_coords({\"latitude\": ([\"y\", \"x\"], lats), \"longitude\": ([\"y\", \"x\"], lons)})\n    ds.latitude.attrs[\"units\"] = \"degrees_north\"\n    ds.longitude.attrs[\"units\"] = \"degrees_east\"\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/utils/","title":"Utils","text":""},{"location":"api/_src/geoprocessing/utils/#rs_tools._src.geoprocessing.utils.check_sat_FOV","title":"<code>check_sat_FOV(region, FOV)</code>","text":"<p>Check if the region is within the Field of View (FOV) of the satellite.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>Tuple[int, int, int, int]</code> <p>The region (lon_min, lat_min, lon_max, lat_max) to check if it is within the FOV</p> required <code>FOV</code> <code>Tuple[int, int]</code> <p>The Field of View (FOV) (lon_min, lat_min, lon_max, lat_max) of the satellite.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the region is within the FOV, False otherwise.</p> Source code in <code>rs_tools/_src/geoprocessing/utils.py</code> <pre><code>def check_sat_FOV(region: Tuple[int, int, int, int], FOV: Tuple[int, int, int, int]) -&gt; bool:\n    \"\"\"\n    Check if the region is within the Field of View (FOV) of the satellite.\n\n    Args:\n        region (Tuple[int, int, int, int]): The region (lon_min, lat_min, lon_max, lat_max) to check if it is within the FOV\n        FOV (Tuple[int, int]): The Field of View (FOV) (lon_min, lat_min, lon_max, lat_max) of the satellite.\n\n    Returns:\n        bool: True if the region is within the FOV, False otherwise.\n    \"\"\"\n    # Check if the region is within the Field of View (FOV) of the satellite.\n    if abs(region[0]) &lt;= abs(FOV[0]) and abs(region[1]) &lt;= abs(FOV[1]) and abs(region[2]) &lt;= abs(FOV[2]) and abs(region[3]) &lt;= abs(FOV[3]):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/_src/geoprocessing/utils/#rs_tools._src.geoprocessing.utils.convert_units","title":"<code>convert_units(ds, wavelengths)</code>","text":"<p>Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um. Acts on each band separately.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be converted.</p> required <code>wavelengths</code> <code>Dict[float]</code> <p>Dictionary of wavelengths of data for each band (i).</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The converted dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/utils.py</code> <pre><code>def convert_units(ds: xr.Dataset, wavelengths: Dict) -&gt; xr.Dataset:\n    \"\"\"\n    Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um.\n    Acts on each band separately.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be converted.\n        wavelengths (Dict[float]): Dictionary of wavelengths of data for each band (i).\n\n    Returns:\n        xr.Dataset: The converted dataset.\n    \"\"\"\n    for band in ds.data_vars:\n        ds[band] = ds[band] * 0.001  # to convert mW to W\n        ds[band] = ds[band] * 10000 / wavelengths[band]**2  # to convert cm^-1 to um\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/","title":"Geoprocessor Goes16","text":""},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing","title":"<code>GOES16GeoProcessing</code>  <code>dataclass</code>","text":"<p>A class for geoprocessing GOES-16 data.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>float</code> <p>The resolution in meters.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <code>region</code> <code>Tuple[str]</code> <p>The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).</p> <code>resample_method</code> <code>str</code> <p>The resampling method to use.</p> <p>Methods:</p> Name Description <code>goes_files</code> <p>Returns a list of all GOES files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>@dataclass\nclass GOES16GeoProcessing:\n    \"\"\"\n    A class for geoprocessing GOES-16 data.\n\n    Attributes:\n        resolution (float): The resolution in meters.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n        region (Tuple[str]): The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).\n        resample_method (str): The resampling method to use.\n\n    Methods:\n        goes_files(self) -&gt; List[str]: Returns a list of all GOES files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    resolution: float\n    read_path: str\n    save_path: str\n    region: Optional[Tuple[int, int, int, int]]\n    resample_method: str\n\n    @property\n    def goes_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all GOES files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all GOES files from specified path\n        files = get_list_filenames(self.read_path, \".nc\")\n        return files\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # convert measurement angles to horizontal distance in meters\n        ds = correct_goes16_satheight(ds) \n        try:\n            # correct band coordinates to reorganize xarray dataset\n            ds = correct_goes16_bands(ds) \n        except AttributeError:\n            pass\n        # assign coordinate reference system\n        ds = add_goes16_crs(ds)\n\n        if self.region is not None:\n            logger.info(f\"Subsetting data to region: {self.region}\")\n            # subset data\n            lon_bnds = (self.region[0], self.region[2])\n            lat_bnds = (self.region[1], self.region[3])\n            # convert lat lon bounds to x y (in meters)\n            x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.FOV.crs, lon=lon_bnds, lat=lat_bnds, )\n            # check that region is within the satellite field of view\n            # compile satellite FOV\n            satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n            # compile region bounds in x y\n            region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n            if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n                raise ValueError(\"Region is not within the satellite field of view\")\n\n            ds = ds.sortby(\"x\").sortby(\"y\")\n            # slice based on x y bounds\n            ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n        else:\n            ds_subset = ds\n\n        if self.resolution is not None:\n            logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n            # resampling\n            ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n        # assign coordinates\n        ds_subset = calc_latlon(ds_subset)\n\n        return ds_subset, ds\n\n    def preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the GOES16 radiance dataset.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        variables = [\"Rad\", \"DQF\"] # \"Rad\" = radiance, \"DQF\" = data quality flag\n\n        # do core preprocess function (e.g. to correct band coordinates, subset data, resample, etc.)\n        ds_subset, ds = self.preprocess_fn(ds)\n\n        # select relevant variables\n        ds_subset = ds_subset[variables]\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.t.values) \n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\") \n        # assign bands data to each variable\n        ds_subset[variables] = ds_subset[variables].expand_dims({\"band\": ds.band.values})\n        # attach time coordinate\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n        # assign band attributes to dataset\n        ds_subset.band.attrs = ds.band.attrs\n        # TODO: Correct wavelength assignment. This attaches 16 wavelengths to each band.\n        # assign band wavelength to each variable\n        ds_subset = ds_subset.assign_coords({\"band_wavelength\": ds.band_wavelength.values})\n        ds_subset.band_wavelength.attrs = ds.band_wavelength.attrs\n\n        return ds_subset\n\n    def preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the input dataset for GOES16 cloud masks.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        variables = [\"BCM\"]\n\n        # do core preprocess function\n        ds_subset, ds = self.preprocess_fn(ds)\n\n        # select relevant variable\n        ds_subset = ds_subset[variables]\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.t.values)\n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")\n        # assign time data to variable\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n\n        return ds_subset\n\n    def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain radiance data\n        files = list(filter(lambda x: \"Rad\" in x, files))\n\n        # Check that all 16 bands are present\n        logger.info(f\"Number of radiance files: {len(files)}\")\n        assert len(files) == 16\n\n        # open multiple files as a single dataset\n        ds = [xr.open_mfdataset(ifile, preprocess=self.preprocess_fn_radiances, concat_dim=\"band\", combine=\"nested\") for\n              ifile in files]\n        # reinterpolate to match coordinates of the first image\n        ds = [ds[0]] + [ids.interp(x=ds[0].x, y=ds[0].y) for ids in ds[1:]]\n        # concatenate in new band dimension\n        ds = xr.concat(ds, dim=\"band\")\n\n        # NOTE: Keep only certain relevant attributes\n        attrs_rad = ds[\"Rad\"].attrs\n\n        ds[\"Rad\"].attrs = {}\n        ds[\"Rad\"].attrs = dict(\n            long_name=attrs_rad[\"long_name\"],\n            standard_name=attrs_rad[\"standard_name\"],\n            units=attrs_rad[\"units\"],\n        )\n        ds[\"DQF\"].attrs = {}\n\n        return ds\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain cloud mask data\n        files = list(filter(lambda x: \"ACMF\" in x, files))\n\n        # Check that only one file is present\n        logger.info(f\"Number of cloud mask files: {len(files)}\")\n        assert len(files) == 1\n\n        # open multiple files as a single dataset\n        ds = xr.open_mfdataset(files[0])\n        ds = self.preprocess_fn_cloudmask(ds)\n\n        # NOTE: Keep only certain relevant attributes\n        attrs_bcm = ds[\"BCM\"].attrs\n        ds = ds.rename({\"BCM\": \"cloud_mask\"})\n        ds[\"cloud_mask\"].attrs = {}\n        ds[\"cloud_mask\"].attrs = dict(\n            long_name=attrs_bcm[\"long_name\"],\n            standard_name=attrs_bcm[\"standard_name\"],\n            units=attrs_bcm[\"units\"],\n        )\n\n        return ds\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        unique_times = list(set(map(parse_goes16_dates_from_file, self.goes_files)))\n\n        pbar_time = tqdm(unique_times)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get files from unique times\n            files = list(filter(lambda x: itime in x, self.goes_files))\n\n            try:\n                # load radiances\n                ds = self.preprocess_radiances(files)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing bands\")\n                continue\n            try:\n                # load cloud mask\n                ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n\n            # interpolate cloud mask to data\n            ds_clouds = ds_clouds.interp(x=ds.x, y=ds.y)\n            # save cloud mask as data coordinate\n            ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values.squeeze())})\n            ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            itime_name = format_goes_dates(itime)\n            save_filename = Path(self.save_path).joinpath(f\"{itime_name}_goes16.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n            # save to netcdf\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.goes_files","title":"<code>goes_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all GOES files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain cloud mask data\n    files = list(filter(lambda x: \"ACMF\" in x, files))\n\n    # Check that only one file is present\n    logger.info(f\"Number of cloud mask files: {len(files)}\")\n    assert len(files) == 1\n\n    # open multiple files as a single dataset\n    ds = xr.open_mfdataset(files[0])\n    ds = self.preprocess_fn_cloudmask(ds)\n\n    # NOTE: Keep only certain relevant attributes\n    attrs_bcm = ds[\"BCM\"].attrs\n    ds = ds.rename({\"BCM\": \"cloud_mask\"})\n    ds[\"cloud_mask\"].attrs = {}\n    ds[\"cloud_mask\"].attrs = dict(\n        long_name=attrs_bcm[\"long_name\"],\n        standard_name=attrs_bcm[\"standard_name\"],\n        units=attrs_bcm[\"units\"],\n    )\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    unique_times = list(set(map(parse_goes16_dates_from_file, self.goes_files)))\n\n    pbar_time = tqdm(unique_times)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get files from unique times\n        files = list(filter(lambda x: itime in x, self.goes_files))\n\n        try:\n            # load radiances\n            ds = self.preprocess_radiances(files)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing bands\")\n            continue\n        try:\n            # load cloud mask\n            ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n\n        # interpolate cloud mask to data\n        ds_clouds = ds_clouds.interp(x=ds.x, y=ds.y)\n        # save cloud mask as data coordinate\n        ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values.squeeze())})\n        ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        itime_name = format_goes_dates(itime)\n        save_filename = Path(self.save_path).joinpath(f\"{itime_name}_goes16.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n        # save to netcdf\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[Dataset, Dataset]</code> <p>Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # convert measurement angles to horizontal distance in meters\n    ds = correct_goes16_satheight(ds) \n    try:\n        # correct band coordinates to reorganize xarray dataset\n        ds = correct_goes16_bands(ds) \n    except AttributeError:\n        pass\n    # assign coordinate reference system\n    ds = add_goes16_crs(ds)\n\n    if self.region is not None:\n        logger.info(f\"Subsetting data to region: {self.region}\")\n        # subset data\n        lon_bnds = (self.region[0], self.region[2])\n        lat_bnds = (self.region[1], self.region[3])\n        # convert lat lon bounds to x y (in meters)\n        x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.FOV.crs, lon=lon_bnds, lat=lat_bnds, )\n        # check that region is within the satellite field of view\n        # compile satellite FOV\n        satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n        # compile region bounds in x y\n        region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n        if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n            raise ValueError(\"Region is not within the satellite field of view\")\n\n        ds = ds.sortby(\"x\").sortby(\"y\")\n        # slice based on x y bounds\n        ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n    else:\n        ds_subset = ds\n\n    if self.resolution is not None:\n        logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n        # resampling\n        ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n    # assign coordinates\n    ds_subset = calc_latlon(ds_subset)\n\n    return ds_subset, ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(ds)</code>","text":"<p>Preprocesses the input dataset for GOES16 cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the input dataset for GOES16 cloud masks.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    variables = [\"BCM\"]\n\n    # do core preprocess function\n    ds_subset, ds = self.preprocess_fn(ds)\n\n    # select relevant variable\n    ds_subset = ds_subset[variables]\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.t.values)\n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")\n    # assign time data to variable\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(ds)</code>","text":"<p>Preprocesses the GOES16 radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the GOES16 radiance dataset.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    variables = [\"Rad\", \"DQF\"] # \"Rad\" = radiance, \"DQF\" = data quality flag\n\n    # do core preprocess function (e.g. to correct band coordinates, subset data, resample, etc.)\n    ds_subset, ds = self.preprocess_fn(ds)\n\n    # select relevant variables\n    ds_subset = ds_subset[variables]\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.t.values) \n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\") \n    # assign bands data to each variable\n    ds_subset[variables] = ds_subset[variables].expand_dims({\"band\": ds.band.values})\n    # attach time coordinate\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n    # assign band attributes to dataset\n    ds_subset.band.attrs = ds.band.attrs\n    # TODO: Correct wavelength assignment. This attaches 16 wavelengths to each band.\n    # assign band wavelength to each variable\n    ds_subset = ds_subset.assign_coords({\"band_wavelength\": ds.band_wavelength.values})\n    ds_subset.band_wavelength.attrs = ds.band_wavelength.attrs\n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain radiance data\n    files = list(filter(lambda x: \"Rad\" in x, files))\n\n    # Check that all 16 bands are present\n    logger.info(f\"Number of radiance files: {len(files)}\")\n    assert len(files) == 16\n\n    # open multiple files as a single dataset\n    ds = [xr.open_mfdataset(ifile, preprocess=self.preprocess_fn_radiances, concat_dim=\"band\", combine=\"nested\") for\n          ifile in files]\n    # reinterpolate to match coordinates of the first image\n    ds = [ds[0]] + [ids.interp(x=ds[0].x, y=ds[0].y) for ids in ds[1:]]\n    # concatenate in new band dimension\n    ds = xr.concat(ds, dim=\"band\")\n\n    # NOTE: Keep only certain relevant attributes\n    attrs_rad = ds[\"Rad\"].attrs\n\n    ds[\"Rad\"].attrs = {}\n    ds[\"Rad\"].attrs = dict(\n        long_name=attrs_rad[\"long_name\"],\n        standard_name=attrs_rad[\"standard_name\"],\n        units=attrs_rad[\"units\"],\n    )\n    ds[\"DQF\"].attrs = {}\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.geoprocess","title":"<code>geoprocess(resolution=None, read_path='./', save_path='./', region=None, resample_method='bilinear')</code>","text":"<p>Geoprocesses GOES 16 files</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>float</code> <p>The resolution in meters to resample data to. Defaults to None.</p> <code>None</code> <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <code>region</code> <code>str</code> <p>The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.</p> <code>None</code> <code>resample_method</code> <code>str</code> <p>The resampling method to use. Defaults to \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def geoprocess(\n        resolution: float = None, #\u00a0defined in meters\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        region: str = None,\n        resample_method: str = \"bilinear\",\n):\n    \"\"\"\n    Geoprocesses GOES 16 files\n\n    Args:\n        resolution (float, optional): The resolution in meters to resample data to. Defaults to None.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n        region (str, optional): The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.\n        resample_method (str, optional): The resampling method to use. Defaults to \"bilinear\".\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize GOES 16 GeoProcessor\n    logger.info(f\"Initializing GOES16 GeoProcessor...\")\n    # Extracting region from str\n    if region is not None:\n        region = tuple(map(lambda x: int(x), region.split(\" \")))\n\n    goes16_geoprocessor = GOES16GeoProcessing(\n        resolution=resolution, \n        read_path=read_path, \n        save_path=save_path,\n        region=region,\n        resample_method=resample_method\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    goes16_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished GOES 16 GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/goes/interp/#rs_tools._src.geoprocessing.goes.interp.resample_rioxarray","title":"<code>resample_rioxarray(ds, resolution=1000, method='bilinear')</code>","text":"<p>Resamples a raster dataset using rasterio-xarray.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be resampled.</p> required <code>resolution</code> <code>int</code> <p>The desired resolution of the resampled dataset. Default is 1_000.</p> <code>1000</code> <code>method</code> <code>str</code> <p>The resampling method to be used. Default is \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The resampled dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/interp.py</code> <pre><code>def resample_rioxarray(ds: xr.Dataset, resolution: int=1_000, method: str=\"bilinear\") -&gt; xr.Dataset:\n    \"\"\"\n    Resamples a raster dataset using rasterio-xarray.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be resampled.\n        resolution (int): The desired resolution of the resampled dataset. Default is 1_000.\n        method (str): The resampling method to be used. Default is \"bilinear\".\n\n    Returns:\n        xr.Dataset: The resampled dataset.\n    \"\"\"\n\n    ds = ds.rio.reproject(\n        ds.rio.crs,\n        resolution=resolution,\n        resample=rioxarray_samplers[method], \n    )\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/goes/reproject/#rs_tools._src.geoprocessing.goes.reproject.add_goes16_crs","title":"<code>add_goes16_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given GOES16 dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/reproject.py</code> <pre><code>def add_goes16_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given GOES16 dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n\n    # load CRS\n    cc = CRS.from_cf(ds.goes_imager_projection.attrs)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc.to_string(), inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/validation/","title":"Validation","text":""},{"location":"api/_src/geoprocessing/goes/validation/#rs_tools._src.geoprocessing.goes.validation.correct_goes16_bands","title":"<code>correct_goes16_bands(ds)</code>","text":"<p>Corrects the band coordinates in a GOES-16 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset containing GOES-16 bands.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The corrected dataset with updated band coordinates.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/validation.py</code> <pre><code>def correct_goes16_bands(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Corrects the band coordinates in a GOES-16 dataset.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset containing GOES-16 bands.\n\n    Returns:\n        xr.Dataset: The corrected dataset with updated band coordinates.\n\n    \"\"\"\n    # reassign coordinate\n    band_id_attrs = ds.band_id.attrs\n    ds = ds.assign_coords(band=ds.band_id.values)\n    ds.band.attrs = band_id_attrs\n\n    # drop bandid dims\n    ds = ds.drop_vars(\"band_id\")\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/validation/#rs_tools._src.geoprocessing.goes.validation.correct_goes16_satheight","title":"<code>correct_goes16_satheight(ds)</code>","text":"<p>Convert measurement angle of GOES-16 satellite data to horizontal distance (in meters).</p> <p>Parameters: - ds (xr.Dataset): The input dataset containing the GOES-16 satellite data.</p> <p>Returns: - xr.Dataset: The dataset with corrected perspective height.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/validation.py</code> <pre><code>def correct_goes16_satheight(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Convert measurement angle of GOES-16 satellite data to horizontal distance (in meters).\n\n    Parameters:\n    - ds (xr.Dataset): The input dataset containing the GOES-16 satellite data.\n\n    Returns:\n    - xr.Dataset: The dataset with corrected perspective height.\n    \"\"\"\n\n    # get perspective height\n    sat_height = ds.goes_imager_projection.attrs[\"perspective_point_height\"]\n\n    # reassign coordinates to correct height\n    x_attrs = ds.x.attrs\n    ds = ds.assign_coords({\"x\": ds.x.values * sat_height})\n    ds[\"x\"].attrs = x_attrs\n    ds[\"x\"].attrs[\"units\"] = \"meters\"\n\n    y_attrs = ds.y.attrs\n    ds = ds.assign_coords({\"y\": ds.y.values * sat_height})\n    ds[\"y\"].attrs = y_attrs\n    ds[\"y\"].attrs[\"units\"] = \"meters\"\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/","title":"Geoprocessor Modis","text":""},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing","title":"<code>MODISGeoProcessing</code>  <code>dataclass</code>","text":"Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>@dataclass\nclass MODISGeoProcessing:\n    satellite: str\n    read_path: str\n    save_path: str\n    \"\"\"\n    A class for geoprocessing MODIS data.\n\n    Attributes:\n        satellite (str): The satellite to geoprocess data for.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n\n    Methods:\n        modis_files(self) -&gt; List[str]: Returns a list of all MODIS files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    @property\n    def modis_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all MODIS files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all MODIS filenames within the path\n        files = get_list_filenames(self.read_path, \".hdf\")\n        return files\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            ds (xr.Dataset):: The preprocessed dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # assign coordinate reference system\n        ds = add_modis_crs(ds)\n\n        # TODO: Add functionality to resample data to specific resolution\n\n        return ds\n\n    def preprocess_fn_radiances(self, file: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the MODIS radiance dataset.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"modis_l1b\",\n            filenames=file\n        )\n        # Load radiance bands\n        channels = get_modis_channel_numbers()\n        scn.load(channels, generate=False, calibration='radiance')\n\n        # change to xarray data\n        ds = scn.to_xarray_dataset()  \n\n        # do core preprocess function (e.g. resample, add crs etc.)\n        ds = self.preprocess_fn(ds) \n\n        # Store the attributes in a dict before concatenation\n        attrs_dict = {x: ds[x].attrs for x in channels}\n\n        # concatinate in new band dimension, and defining a new variable name\n        # NOTE: Concatination overwrites attrs of bands.\n        ds = ds.assign(Rad=xr.concat(list(map(lambda x: ds[x], channels)), dim=\"band\"))\n        # drop duplicate variables\n        ds = ds.drop(list(map(lambda x: x, channels)))\n        # rename band dimensions\n        ds = ds.assign_coords(band=list(map(lambda x: x, channels)))\n\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.attrs['start_time'])\n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")  \n        # assign bands and time data to each variable\n        ds = ds.assign_coords({\"time\": [time_stamp]})\n\n        # NOTE: Keep only certain relevant attributes\n        ds.attrs = {}\n        ds.attrs = dict(\n            calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n            standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n            platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n            sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n            units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n        )\n\n        # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n        # assign band wavelengths \n        ds = ds.assign_coords({\"band_wavelength\": list(MODIS_WAVELENGTHS.values())})   \n\n        return ds\n\n    def preprocess_fn_cloudmask(self, file: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the input dataset for MODIS cloud masks.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"    \n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"modis_l2\",\n            filenames=file\n        )\n        # Load cloud mask data\n        datasets = scn.available_dataset_names()\n        # Needs to be loaded at 1000 m resolution for all channels to match\n        scn.load(datasets, generate=False, resolution=1000) \n\n        # change to xarray data\n        ds = scn.to_xarray_dataset()\n\n        return ds\n\n    def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        identifier = MODIS_NAME_TO_ID[self.satellite]\n\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: identifier in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of radiance files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_radiances(file)\n\n        return ds\n\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        identifier = MODIS_NAME_TO_ID[f'{self.satellite}_cloud']\n\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: identifier in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of cloud mask files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_cloudmask(file)\n\n        return ds\n\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        unique_times = list(set(map(parse_modis_dates_from_file, self.modis_files)))\n\n        pbar_time = tqdm(unique_times)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get files from unique times\n            files = list(filter(lambda x: itime in x, self.modis_files))\n\n            try:\n                # load radiances\n                ds = self.preprocess_radiances(files)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to error loading\")\n                continue\n            try:\n                # load cloud mask\n                ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n\n            # save cloud mask as data coordinate\n            ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values)})\n            # add cloud mask attrs to dataset\n            ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n            # remove crs from dataset\n            ds = ds.drop_vars(\"crs\")\n            ds = ds.drop_vars(\"spatial_ref\")\n\n            # remove attrs that cause netcdf error\n            for attr in [\"start_time\", \"end_time\", \"area\", \"_satpy_id\"]:\n                ds[\"cloud_mask\"].attrs.pop(attr)\n\n            for var in ds.data_vars:\n                ds[var].attrs = {}\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            itime_name = format_modis_dates(itime)\n            save_filename = Path(self.save_path).joinpath(f\"{itime_name}_{self.satellite}.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n\n            # save to netcdf\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.modis_files","title":"<code>modis_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all MODIS files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.save_path","title":"<code>save_path: str</code>  <code>instance-attribute</code>","text":"<p>A class for geoprocessing MODIS data.</p> <p>Attributes:</p> Name Type Description <code>satellite</code> <code>str</code> <p>The satellite to geoprocess data for.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <p>Functions:</p> Name Description <code>modis_files</code> <p>Returns a list of all MODIS files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    identifier = MODIS_NAME_TO_ID[f'{self.satellite}_cloud']\n\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: identifier in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of cloud mask files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_cloudmask(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    unique_times = list(set(map(parse_modis_dates_from_file, self.modis_files)))\n\n    pbar_time = tqdm(unique_times)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get files from unique times\n        files = list(filter(lambda x: itime in x, self.modis_files))\n\n        try:\n            # load radiances\n            ds = self.preprocess_radiances(files)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to error loading\")\n            continue\n        try:\n            # load cloud mask\n            ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n\n        # save cloud mask as data coordinate\n        ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values)})\n        # add cloud mask attrs to dataset\n        ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n        # remove crs from dataset\n        ds = ds.drop_vars(\"crs\")\n        ds = ds.drop_vars(\"spatial_ref\")\n\n        # remove attrs that cause netcdf error\n        for attr in [\"start_time\", \"end_time\", \"area\", \"_satpy_id\"]:\n            ds[\"cloud_mask\"].attrs.pop(attr)\n\n        for var in ds.data_vars:\n            ds[var].attrs = {}\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        itime_name = format_modis_dates(itime)\n        save_filename = Path(self.save_path).joinpath(f\"{itime_name}_{self.satellite}.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n\n        # save to netcdf\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Name Type Description <code>ds</code> <code>Dataset</code> <p>: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        ds (xr.Dataset):: The preprocessed dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # assign coordinate reference system\n    ds = add_modis_crs(ds)\n\n    # TODO: Add functionality to resample data to specific resolution\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(file)</code>","text":"<p>Preprocesses the input dataset for MODIS cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn_cloudmask(self, file: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the input dataset for MODIS cloud masks.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"    \n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"modis_l2\",\n        filenames=file\n    )\n    # Load cloud mask data\n    datasets = scn.available_dataset_names()\n    # Needs to be loaded at 1000 m resolution for all channels to match\n    scn.load(datasets, generate=False, resolution=1000) \n\n    # change to xarray data\n    ds = scn.to_xarray_dataset()\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(file)</code>","text":"<p>Preprocesses the MODIS radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn_radiances(self, file: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the MODIS radiance dataset.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"modis_l1b\",\n        filenames=file\n    )\n    # Load radiance bands\n    channels = get_modis_channel_numbers()\n    scn.load(channels, generate=False, calibration='radiance')\n\n    # change to xarray data\n    ds = scn.to_xarray_dataset()  \n\n    # do core preprocess function (e.g. resample, add crs etc.)\n    ds = self.preprocess_fn(ds) \n\n    # Store the attributes in a dict before concatenation\n    attrs_dict = {x: ds[x].attrs for x in channels}\n\n    # concatinate in new band dimension, and defining a new variable name\n    # NOTE: Concatination overwrites attrs of bands.\n    ds = ds.assign(Rad=xr.concat(list(map(lambda x: ds[x], channels)), dim=\"band\"))\n    # drop duplicate variables\n    ds = ds.drop(list(map(lambda x: x, channels)))\n    # rename band dimensions\n    ds = ds.assign_coords(band=list(map(lambda x: x, channels)))\n\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.attrs['start_time'])\n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")  \n    # assign bands and time data to each variable\n    ds = ds.assign_coords({\"time\": [time_stamp]})\n\n    # NOTE: Keep only certain relevant attributes\n    ds.attrs = {}\n    ds.attrs = dict(\n        calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n        standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n        platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n        sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n        units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n    )\n\n    # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n    # assign band wavelengths \n    ds = ds.assign_coords({\"band_wavelength\": list(MODIS_WAVELENGTHS.values())})   \n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    identifier = MODIS_NAME_TO_ID[self.satellite]\n\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: identifier in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of radiance files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_radiances(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.geoprocess","title":"<code>geoprocess(satellite, read_path='./', save_path='./')</code>","text":"<p>Geoprocesses MODIS files</p> <p>Parameters:</p> Name Type Description Default <code>satellite</code> <code>str</code> <p>The satellite of the data to geoprocess.</p> required <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def geoprocess(\n        satellite: str,\n        read_path: str = \"./\",\n        save_path: str = \"./\"\n):\n    \"\"\"\n    Geoprocesses MODIS files\n\n    Args:\n        satellite (str, optional): The satellite of the data to geoprocess.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize MODIS GeoProcessor\n    logger.info(f\"Initializing {satellite.upper()} GeoProcessor...\")\n\n    modis_geoprocessor = MODISGeoProcessing(\n        satellite=satellite, \n        read_path=read_path, \n        save_path=save_path\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    modis_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished {satellite.upper()} GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/modis/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/modis/reproject/#rs_tools._src.geoprocessing.modis.reproject.add_modis_crs","title":"<code>add_modis_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given MODIS dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/reproject.py</code> <pre><code>def add_modis_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given MODIS dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n    # define CRS of MODIS dataset\n    crs = 'WGS84'\n\n    # load source CRS from the WKT string\n    cc = CRS(crs)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc, inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/rescale/","title":"Rescale","text":""},{"location":"api/_src/geoprocessing/modis/rescale/#rs_tools._src.geoprocessing.modis.rescale.convert_integers2radiances","title":"<code>convert_integers2radiances(da)</code>","text":"<p>Function to convert scaled integers to radiances. Scaled integers are unitless, radiances are given in W^2/m^2/um/sr</p> Source code in <code>rs_tools/_src/geoprocessing/modis/rescale.py</code> <pre><code>def convert_integers2radiances(\n        da: xr.DataArray, \n    ) -&gt; xr.DataArray:\n    \"\"\"\n    Function to convert scaled integers to radiances.\n    Scaled integers are unitless, radiances are given in W^2/m^2/um/sr\n    \"\"\"\n\n    radiance_scale = da.radiance_scales\n    radiance_offsets = da.radiance_offsets \n\n    radiance_scale = np.expand_dims(radiance_scale, axis=(1,2))\n    radiance_offsets = np.expand_dims(radiance_offsets, axis=(1,2))\n\n    assert radiance_offsets.shape == radiance_scale.shape\n\n    corrected_data = (da - radiance_offsets)*radiance_scale\n\n    #TODO - change attributes to have units\n    #TODO - change attributes to change name of variable\n    #TODO - change attributes to change rescaling\n\n    return corrected_data\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/rescale/#rs_tools._src.geoprocessing.modis.rescale.convert_integers2reflectances","title":"<code>convert_integers2reflectances(da)</code>","text":"<p>Function to convert scaled integers to reflectances. Scaled integers and reflectances are both unitless.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/rescale.py</code> <pre><code>def convert_integers2reflectances(\n        da: xr.DataArray, \n    ) -&gt; xr.DataArray:\n    \"\"\"\n    Function to convert scaled integers to reflectances.\n    Scaled integers and reflectances are both unitless.\n    \"\"\"\n\n    reflectance_scale = da.reflectance_scales\n    reflectance_offsets = da.reflectance_offsets \n\n    reflectance_scale = np.expand_dims(reflectance_scale, axis=(1,2))\n    reflectance_offsets = np.expand_dims(reflectance_offsets, axis=(1,2))\n\n    assert reflectance_offsets.shape == reflectance_scale.shape\n\n    corrected_data = (da - reflectance_offsets)*reflectance_scale\n\n    return corrected_data\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/","title":"Geoprocessor Msg","text":""},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing","title":"<code>MSGGeoProcessing</code>  <code>dataclass</code>","text":"<p>A class for geoprocessing MSG data.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>float</code> <p>The resolution in meters.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <code>region</code> <code>Tuple[str]</code> <p>The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).</p> <code>resample_method</code> <code>str</code> <p>The resampling method to use.</p> <p>Methods:</p> Name Description <code>msg_files</code> <p>Returns a list of all MSG files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>@dataclass\nclass MSGGeoProcessing:\n    \"\"\"\n    A class for geoprocessing MSG data.\n\n    Attributes:\n        resolution (float): The resolution in meters.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n        region (Tuple[str]): The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).\n        resample_method (str): The resampling method to use.\n\n    Methods:\n        msg_files(self) -&gt; List[str]: Returns a list of all MSG files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    resolution: float\n    read_path: str\n    save_path: str\n    region: Optional[Tuple[int, int, int, int]]\n    resample_method: str\n\n    @property\n    def msg_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all MSG files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all MSG radiance files from specified path\n        files_radiances = get_list_filenames(self.read_path, \".nat\")\n        # get a list of all MSG cloud mask files from specified path\n        files_cloudmask = get_list_filenames(self.read_path, \".grb\")\n        return files_radiances, files_cloudmask\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # assign coordinate reference system\n        ds = add_msg_crs(ds)\n\n        if self.region is not None:\n            logger.info(f\"Subsetting data to region: {self.region}\")\n            # subset data\n            lon_bnds = (self.region[0], self.region[2])\n            lat_bnds = (self.region[1], self.region[3])\n            # convert lat lon bounds to x y (in meters)\n            x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.rio.crs, lon=lon_bnds, lat=lat_bnds, )\n            # check that region is within the satellite field of view\n            # compile satellite FOV\n            satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n            # compile region bounds in x y\n            region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n            if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n                raise ValueError(\"Region is not within the satellite field of view\")\n\n            ds = ds.sortby(\"x\").sortby(\"y\")\n            # slice based on x y bounds\n            ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n        else:\n            ds_subset = ds\n\n        if self.resolution is not None:\n            logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n            # resampling\n            ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n        # assign coordinates\n        ds_subset = calc_latlon(ds_subset)\n\n        return ds_subset, ds\n\n    def preprocess_fn_radiances(self, file: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the MSG radiance dataset.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n\n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"seviri_l1b_native\",\n            filenames=file\n        )\n        # Load radiance bands\n        channels = [x for x in scn.available_dataset_names() if x!='HRV']\n        assert len(channels) == 11, \"Number of channels is not 11\"\n\n        scn.load(channels, generate=False, calibration='radiance')\n\n        # change to xarray data\n        ds = scn.to_xarray()\n\n        # attach cloud mask as data variable before preprocessing\n        ds = ds.assign(cloud_mask=((\"y\", \"x\"), cloud_mask))\n\n        # reset coordinates for resampling/reprojecting\n        # this drops all {channel}_acq_time coordinates\n        ds = ds.reset_coords(drop=True)\n\n        # do core preprocess function (e.g. resample, add crs etc.)\n        ds_subset, ds = self.preprocess_fn(ds) \n\n        # Store the attributes in a dict before concatenation\n        attrs_dict = {x: ds_subset[x].attrs for x in channels}\n\n        # concatinate in new band dimension\n        # NOTE: Concatination overwrites attrs of bands.\n        ds_subset = ds_subset.assign(Rad=xr.concat(list(map(lambda x: ds_subset[x], channels)), dim=\"band\"))\n        # rename band dimensions\n        ds_subset = ds_subset.assign_coords(band=list(map(lambda x: x, channels)))\n\n        # re-index coordinates\n        ds_subset = ds_subset.set_coords(['latitude', 'longitude', 'cloud_mask'])\n\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop(list(map(lambda x: x, channels)))\n\n        # extract measurement time\n        time_stamp = attrs_dict[list(attrs_dict.keys())[0]]['start_time']\n        # assign bands and time data to each variable\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n\n        # NOTE: Keep only certain relevant attributes\n        ds_subset.attrs = {}\n        ds_subset.attrs = dict(\n            calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n            standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n            platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n            sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n            units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n            orbital_parameters=attrs_dict[list(attrs_dict.keys())[0]][\"orbital_parameters\"]\n        )\n\n        # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n        # assign band wavelengths \n        ds_subset = ds_subset.assign_coords({\"band_wavelength\": list(MSG_WAVELENGTHS.values())}) \n\n        return ds_subset\n\n    def preprocess_fn_cloudmask(self, file: List[str]) -&gt; np.array:\n        \"\"\"\n        Preprocesses the input dataset for MSG cloud masks.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            np.array: The preprocessed cloud mask dataset.\n        \"\"\"\n\n        grbs = pygrib.open(file[0])\n        # Loop over all messages in the GRIB file\n        for grb in grbs:\n            if grb.name == 'Cloud mask':\n                # Extract values from grb and return np.array\n                cloud_mask = grb.values\n                return cloud_mask\n\n    def preprocess_radiances(self, files: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: \".nat\" in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of radiance files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_radiances(file, cloud_mask=cloud_mask)\n\n        return ds\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain cloud mask data\n        file = list(filter(lambda x: \"CLMK\" in x, files))\n\n        # Check that only one file is present\n        logger.info(f\"Number of cloud mask files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_cloudmask(file)\n\n        return ds\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        files_radiances, files_cloudmask = self.msg_files\n        unique_times_radiances = list(set(map(parse_msg_dates_from_file, files_radiances)))\n        unique_times_cloudmask = list(set(map(parse_msg_dates_from_file, files_cloudmask)))\n\n        df_matches = match_timestamps(unique_times_radiances, unique_times_cloudmask, cutoff=15) \n\n        pbar_time = tqdm(df_matches[\"timestamps_data\"].values)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get cloud mask file for specific time\n            itime_cloud = df_matches.loc[df_matches[\"timestamps_data\"] == itime, \"timestamps_cloudmask\"].values[0]\n            files_cloud = list(filter(lambda x: itime_cloud in x, files_cloudmask))\n\n            try:\n                # load cloud mask\n                cloud_mask = self.preprocess_cloud_mask(files_cloud)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n\n            # get data files for specific time\n            files = list(filter(lambda x: itime in x, files_radiances))\n\n            try:\n                # load radiances and attach cloud mask\n                ds = self.preprocess_radiances(files, cloud_mask=cloud_mask)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to error loading\")\n                continue\n\n             # remove crs from dataset\n            ds = ds.drop_vars('msg_seviri_fes_3km') \n\n            # remove attrs that cause netcdf error\n            for var in ds.data_vars:\n                ds[var].attrs.pop('grid_mapping', None)\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            save_filename = Path(self.save_path).joinpath(f\"{itime}_msg.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n\n            # save to netcdf\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.msg_files","title":"<code>msg_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all MSG files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain cloud mask data\n    file = list(filter(lambda x: \"CLMK\" in x, files))\n\n    # Check that only one file is present\n    logger.info(f\"Number of cloud mask files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_cloudmask(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    files_radiances, files_cloudmask = self.msg_files\n    unique_times_radiances = list(set(map(parse_msg_dates_from_file, files_radiances)))\n    unique_times_cloudmask = list(set(map(parse_msg_dates_from_file, files_cloudmask)))\n\n    df_matches = match_timestamps(unique_times_radiances, unique_times_cloudmask, cutoff=15) \n\n    pbar_time = tqdm(df_matches[\"timestamps_data\"].values)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get cloud mask file for specific time\n        itime_cloud = df_matches.loc[df_matches[\"timestamps_data\"] == itime, \"timestamps_cloudmask\"].values[0]\n        files_cloud = list(filter(lambda x: itime_cloud in x, files_cloudmask))\n\n        try:\n            # load cloud mask\n            cloud_mask = self.preprocess_cloud_mask(files_cloud)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n\n        # get data files for specific time\n        files = list(filter(lambda x: itime in x, files_radiances))\n\n        try:\n            # load radiances and attach cloud mask\n            ds = self.preprocess_radiances(files, cloud_mask=cloud_mask)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to error loading\")\n            continue\n\n         # remove crs from dataset\n        ds = ds.drop_vars('msg_seviri_fes_3km') \n\n        # remove attrs that cause netcdf error\n        for var in ds.data_vars:\n            ds[var].attrs.pop('grid_mapping', None)\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        save_filename = Path(self.save_path).joinpath(f\"{itime}_msg.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n\n        # save to netcdf\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[Dataset, Dataset]</code> <p>Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # assign coordinate reference system\n    ds = add_msg_crs(ds)\n\n    if self.region is not None:\n        logger.info(f\"Subsetting data to region: {self.region}\")\n        # subset data\n        lon_bnds = (self.region[0], self.region[2])\n        lat_bnds = (self.region[1], self.region[3])\n        # convert lat lon bounds to x y (in meters)\n        x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.rio.crs, lon=lon_bnds, lat=lat_bnds, )\n        # check that region is within the satellite field of view\n        # compile satellite FOV\n        satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n        # compile region bounds in x y\n        region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n        if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n            raise ValueError(\"Region is not within the satellite field of view\")\n\n        ds = ds.sortby(\"x\").sortby(\"y\")\n        # slice based on x y bounds\n        ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n    else:\n        ds_subset = ds\n\n    if self.resolution is not None:\n        logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n        # resampling\n        ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n    # assign coordinates\n    ds_subset = calc_latlon(ds_subset)\n\n    return ds_subset, ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(file)</code>","text":"<p>Preprocesses the input dataset for MSG cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.array: The preprocessed cloud mask dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn_cloudmask(self, file: List[str]) -&gt; np.array:\n    \"\"\"\n    Preprocesses the input dataset for MSG cloud masks.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        np.array: The preprocessed cloud mask dataset.\n    \"\"\"\n\n    grbs = pygrib.open(file[0])\n    # Loop over all messages in the GRIB file\n    for grb in grbs:\n        if grb.name == 'Cloud mask':\n            # Extract values from grb and return np.array\n            cloud_mask = grb.values\n            return cloud_mask\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(file, cloud_mask)</code>","text":"<p>Preprocesses the MSG radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn_radiances(self, file: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the MSG radiance dataset.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n\n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"seviri_l1b_native\",\n        filenames=file\n    )\n    # Load radiance bands\n    channels = [x for x in scn.available_dataset_names() if x!='HRV']\n    assert len(channels) == 11, \"Number of channels is not 11\"\n\n    scn.load(channels, generate=False, calibration='radiance')\n\n    # change to xarray data\n    ds = scn.to_xarray()\n\n    # attach cloud mask as data variable before preprocessing\n    ds = ds.assign(cloud_mask=((\"y\", \"x\"), cloud_mask))\n\n    # reset coordinates for resampling/reprojecting\n    # this drops all {channel}_acq_time coordinates\n    ds = ds.reset_coords(drop=True)\n\n    # do core preprocess function (e.g. resample, add crs etc.)\n    ds_subset, ds = self.preprocess_fn(ds) \n\n    # Store the attributes in a dict before concatenation\n    attrs_dict = {x: ds_subset[x].attrs for x in channels}\n\n    # concatinate in new band dimension\n    # NOTE: Concatination overwrites attrs of bands.\n    ds_subset = ds_subset.assign(Rad=xr.concat(list(map(lambda x: ds_subset[x], channels)), dim=\"band\"))\n    # rename band dimensions\n    ds_subset = ds_subset.assign_coords(band=list(map(lambda x: x, channels)))\n\n    # re-index coordinates\n    ds_subset = ds_subset.set_coords(['latitude', 'longitude', 'cloud_mask'])\n\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop(list(map(lambda x: x, channels)))\n\n    # extract measurement time\n    time_stamp = attrs_dict[list(attrs_dict.keys())[0]]['start_time']\n    # assign bands and time data to each variable\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n\n    # NOTE: Keep only certain relevant attributes\n    ds_subset.attrs = {}\n    ds_subset.attrs = dict(\n        calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n        standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n        platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n        sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n        units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n        orbital_parameters=attrs_dict[list(attrs_dict.keys())[0]][\"orbital_parameters\"]\n    )\n\n    # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n    # assign band wavelengths \n    ds_subset = ds_subset.assign_coords({\"band_wavelength\": list(MSG_WAVELENGTHS.values())}) \n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files, cloud_mask)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_radiances(self, files: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: \".nat\" in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of radiance files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_radiances(file, cloud_mask=cloud_mask)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.geoprocess","title":"<code>geoprocess(resolution=None, read_path='./', save_path='./', region=None, resample_method='bilinear')</code>","text":"<p>Geoprocesses MSG files</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>float</code> <p>The resolution in meters to resample data to. Defaults to None.</p> <code>None</code> <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <code>region</code> <code>str</code> <p>The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.</p> <code>None</code> <code>resample_method</code> <code>str</code> <p>The resampling method to use. Defaults to \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def geoprocess(\n        resolution: float = None, #\u00a0defined in meters\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        region: str = None,\n        resample_method: str = \"bilinear\",\n):\n    \"\"\"\n    Geoprocesses MSG files\n\n    Args:\n        resolution (float, optional): The resolution in meters to resample data to. Defaults to None.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n        region (str, optional): The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.\n        resample_method (str, optional): The resampling method to use. Defaults to \"bilinear\".\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize MSG GeoProcessor\n    logger.info(f\"Initializing MSG GeoProcessor...\")\n    # Extracting region from str\n    if region is not None:\n        region = tuple(map(lambda x: int(x), region.split(\" \")))\n\n    msg_geoprocessor = MSGGeoProcessing(\n        resolution=resolution, \n        read_path=read_path, \n        save_path=save_path,\n        region=region,\n        resample_method=resample_method\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    msg_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished MSG GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.parse_msg_dates_from_file","title":"<code>parse_msg_dates_from_file(file)</code>","text":"<p>Parses the date and time information from a MSG file name.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file name to parse.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The parsed date and time in the format 'YYYYJJJHHMM'.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def parse_msg_dates_from_file(file: str):\n    \"\"\"\n    Parses the date and time information from a MSG file name.\n\n    Args:\n        file (str): The file name to parse.\n\n    Returns:\n        str: The parsed date and time in the format 'YYYYJJJHHMM'.\n    \"\"\"\n    timestamp = Path(file).name.split(\"-\")[-2]\n    timestamp = timestamp.split(\".\")[0]\n    return timestamp\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/msg/reproject/#rs_tools._src.geoprocessing.msg.reproject.add_msg_crs","title":"<code>add_msg_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given MSG dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/reproject.py</code> <pre><code>def add_msg_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given MSG dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n    # the CRS is stored in data variable attributes of \"msg_seviri_fes_3km\"\n    var = \"msg_seviri_fes_3km\"\n    crs_wkt = ds[var].crs_wkt\n\n    # load source CRS from the WKT string\n    cc = CRS(crs_wkt)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc, inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/preprocessing/normalize/","title":"Normalize","text":""},{"location":"api/_src/preprocessing/prepatcher/","title":"Prepatcher","text":""},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher","title":"<code>PrePatcher</code>  <code>dataclass</code>","text":"<p>A class for preprocessing and saving patches from NetCDF files.</p> <p>Attributes:</p> Name Type Description <code>read_path</code> <code>str</code> <p>The path to the directory containing the NetCDF files.</p> <code>save_path</code> <code>str</code> <p>The path to save the patches.</p> <code>patch_size</code> <code>int</code> <p>The size of each patch.</p> <code>stride_size</code> <code>int</code> <p>The stride size for generating patches.</p> <code>nan_cutoff</code> <code>float</code> <p>The cutoff value for allowed NaN count in a patch.</p> <code>save_filetype</code> <code>str</code> <p>The file type to save patches as. Options are [nc, np].</p> <p>Methods:</p> Name Description <code>nc_files</code> <p>Returns a list of all NetCDF filenames in the read_path directory.</p> <code>save_patches</code> <p>Preprocesses and saves patches from the NetCDF files.</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>@dataclass(frozen=True)\nclass PrePatcher:\n    \"\"\"\n    A class for preprocessing and saving patches from NetCDF files.\n\n    Attributes:\n        read_path (str): The path to the directory containing the NetCDF files.\n        save_path (str): The path to save the patches.\n        patch_size (int): The size of each patch.\n        stride_size (int): The stride size for generating patches.\n        nan_cutoff (float): The cutoff value for allowed NaN count in a patch.\n        save_filetype (str): The file type to save patches as. Options are [nc, np].\n\n    Methods:\n        nc_files(self) -&gt; List[str]: Returns a list of all NetCDF filenames in the read_path directory.\n        save_patches(self): Preprocesses and saves patches from the NetCDF files.\n    \"\"\"\n\n    read_path: str\n    save_path: str \n    patch_size: int\n    stride_size: int \n    nan_cutoff: float\n    save_filetype: str\n\n    @property\n    def nc_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all NetCDF filenames in the read_path directory.\n\n        Returns:\n            List[str]: A list of NetCDF filenames.\n        \"\"\"\n        # get list of all filenames within the path\n        files = get_list_filenames(self.read_path, \".nc\")\n        return files\n\n    def save_patches(self):\n        \"\"\"\n        Preprocesses and saves patches from the NetCDF files.\n        \"\"\"\n        pbar = tqdm(self.nc_files)\n\n        for ifile in pbar:\n            # extract &amp; log timestamp\n            itime = str(Path(ifile).name).split(\"_\")[0]\n            pbar.set_description(f\"Processing: {itime}\")\n            # open dataset\n            ds = xr.open_dataset(ifile, engine=\"netcdf4\")\n            # extract radiance data array\n            da = ds.Rad\n            # define patch parameters\n            patches = dict(x=self.patch_size, y=self.patch_size)\n            strides = dict(x=self.stride_size, y=self.stride_size)\n            # start patching\n            patcher = XRDAPatcher(da=da, patches=patches, strides=strides)\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            for i, ipatch in tqdm(enumerate(patcher), total=len(patcher)):\n                data = ipatch.data # extract data patch\n                if _check_nan_count(data, self.nan_cutoff):\n                    if self.save_filetype == \"nc\":\n                        # reconvert to dataset to attach band_wavelength and time\n                        ipatch = ipatch.to_dataset(name='Rad')\n                        ipatch = ipatch.assign_coords({'time': ds.time.values})\n                        ipatch = ipatch.assign_coords({'band_wavelength': ds.band_wavelength.values})\n                        # compile filename\n                        file_path = Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\")\n                        # remove file if it already exists\n                        if os.path.exists(file_path):\n                            os.remove(file_path)\n                        # save patch to netcdf                  \n                        ipatch.to_netcdf(Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\"), engine=\"netcdf4\")\n                    elif self.save_filetype == \"np\":\n                        # save as numpy files\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_radiance_patch_{i}\"), data)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_latitude_patch_{i}\"), ipatch.latitude.values)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_longitude_patch_{i}\"), ipatch.longitude.values)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_cloudmask_patch_{i}\"), ipatch.cloud_mask.values)\n                else:\n                    logger.info(f'NaN count exceeded for patch {i} of timestamp {itime}.')\n</code></pre>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher.nc_files","title":"<code>nc_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all NetCDF filenames in the read_path directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of NetCDF filenames.</p>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher.save_patches","title":"<code>save_patches()</code>","text":"<p>Preprocesses and saves patches from the NetCDF files.</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>def save_patches(self):\n    \"\"\"\n    Preprocesses and saves patches from the NetCDF files.\n    \"\"\"\n    pbar = tqdm(self.nc_files)\n\n    for ifile in pbar:\n        # extract &amp; log timestamp\n        itime = str(Path(ifile).name).split(\"_\")[0]\n        pbar.set_description(f\"Processing: {itime}\")\n        # open dataset\n        ds = xr.open_dataset(ifile, engine=\"netcdf4\")\n        # extract radiance data array\n        da = ds.Rad\n        # define patch parameters\n        patches = dict(x=self.patch_size, y=self.patch_size)\n        strides = dict(x=self.stride_size, y=self.stride_size)\n        # start patching\n        patcher = XRDAPatcher(da=da, patches=patches, strides=strides)\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        for i, ipatch in tqdm(enumerate(patcher), total=len(patcher)):\n            data = ipatch.data # extract data patch\n            if _check_nan_count(data, self.nan_cutoff):\n                if self.save_filetype == \"nc\":\n                    # reconvert to dataset to attach band_wavelength and time\n                    ipatch = ipatch.to_dataset(name='Rad')\n                    ipatch = ipatch.assign_coords({'time': ds.time.values})\n                    ipatch = ipatch.assign_coords({'band_wavelength': ds.band_wavelength.values})\n                    # compile filename\n                    file_path = Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\")\n                    # remove file if it already exists\n                    if os.path.exists(file_path):\n                        os.remove(file_path)\n                    # save patch to netcdf                  \n                    ipatch.to_netcdf(Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\"), engine=\"netcdf4\")\n                elif self.save_filetype == \"np\":\n                    # save as numpy files\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_radiance_patch_{i}\"), data)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_latitude_patch_{i}\"), ipatch.latitude.values)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_longitude_patch_{i}\"), ipatch.longitude.values)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_cloudmask_patch_{i}\"), ipatch.cloud_mask.values)\n            else:\n                logger.info(f'NaN count exceeded for patch {i} of timestamp {itime}.')\n</code></pre>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.prepatch","title":"<code>prepatch(read_path='./', save_path='./', patch_size=256, stride_size=256, nan_cutoff=0.5, save_filetype='nc')</code>","text":"<p>Patches satellite data into smaller patches for training. Args:     read_path (str, optional): The path to read the input files from. Defaults to \"./\".     save_path (str, optional): The path to save the extracted patches. Defaults to \"./\".     patch_size (int, optional): The size of each patch. Defaults to 256.     stride_size (int, optional): The stride size for patch extraction. Defaults to 256.     nan_cutoff (float): The cutoff value for allowed NaN count in a patch. Defaults to 0.1.     save_filetype (str, optional): The file type to save patches as. Options are [nc, np]</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>def prepatch(\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        patch_size: int = 256,\n        stride_size: int = 256,\n        nan_cutoff: float = 0.5, \n        save_filetype: str = 'nc'\n):\n    \"\"\"\n    Patches satellite data into smaller patches for training.\n    Args:\n        read_path (str, optional): The path to read the input files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the extracted patches. Defaults to \"./\".\n        patch_size (int, optional): The size of each patch. Defaults to 256.\n        stride_size (int, optional): The stride size for patch extraction. Defaults to 256.\n        nan_cutoff (float): The cutoff value for allowed NaN count in a patch. Defaults to 0.1.\n        save_filetype (str, optional): The file type to save patches as. Options are [nc, np]\n\n    Returns:\n        None\n    \"\"\"\n    _check_filetype(file_type=save_filetype)\n\n    # Initialize Prepatcher\n    logger.info(f\"Initializing Prepatcher...\")\n    prepatcher = PrePatcher(\n        read_path=read_path, \n        save_path=save_path,\n        patch_size=patch_size,\n        stride_size=stride_size,\n        nan_cutoff=nan_cutoff,\n        save_filetype=save_filetype\n        )\n    logger.info(f\"Patching Files...: {save_path}\")\n    prepatcher.save_patches()\n\n    logger.info(f\"Finished Prepatching Script...!\")\n</code></pre>"},{"location":"api/_src/utils/io/","title":"Io","text":""},{"location":"api/_src/utils/io/#rs_tools._src.utils.io.get_list_filenames","title":"<code>get_list_filenames(data_path='./', ext='*')</code>","text":"<p>Loads a list of file names within a directory</p> Source code in <code>rs_tools/_src/utils/io.py</code> <pre><code>def get_list_filenames(data_path: str=\"./\", ext: str=\"*\"):\n    \"\"\"Loads a list of file names within a directory\n    \"\"\"\n    pattern = f\"*{ext}\"\n    return sorted(glob.glob(os.path.join(data_path, \"**\", pattern), recursive=True))\n</code></pre>"},{"location":"api/_src/utils/math/","title":"Math","text":""},{"location":"api/_src/utils/math/#rs_tools._src.utils.math.bounds_and_points_to_step","title":"<code>bounds_and_points_to_step(xmin, xmax, Nx)</code>","text":"<p>Calculates the dx from the minmax Eq:     Lx = abs(xmax - xmin)     dx = Lx / (Nx - 1)</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Array | float</code> <p>the input start point</p> required <code>xmax</code> <code>Array | float</code> <p>the input end point</p> required <code>Nx</code> <code>int | float</code> <p>the number of points</p> required <p>Returns:</p> Name Type Description <code>dx</code> <code>Array | float</code> <p>the distance between each of the steps.</p> Source code in <code>rs_tools/_src/utils/math.py</code> <pre><code>def bounds_and_points_to_step(xmin: float, xmax: float, Nx: float) -&gt; float:\n    \"\"\"Calculates the dx from the minmax\n    Eq:\n        Lx = abs(xmax - xmin)\n        dx = Lx / (Nx - 1)\n\n    Args:\n        xmin (Array | float): the input start point\n        xmax (Array | float): the input end point\n        Nx (int | float): the number of points\n\n    Returns:\n        dx (Array | float): the distance between each of the\n            steps.\n    \"\"\"\n    Lx = abs(float(xmax) - float(xmin))\n    return float(Lx) / (float(Nx) - 1.0)\n</code></pre>"},{"location":"api/_src/utils/math/#rs_tools._src.utils.math.bounds_and_step_to_points","title":"<code>bounds_and_step_to_points(xmin, xmax, dx)</code>","text":"<p>Calculates the number of points from the endpoints and the stepsize</p> Eq <p>Nx = 1 + floor((xmax-xmin)) / dx)</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Array | float</code> <p>the input start point</p> required <code>xmax</code> <code>Array | float</code> <p>the input end point</p> required <code>dx</code> <code>Array | float</code> <p>stepsize between each point.</p> required <p>Returns:</p> Name Type Description <code>Nx</code> <code>Array | int</code> <p>the number of points</p> Source code in <code>rs_tools/_src/utils/math.py</code> <pre><code>def bounds_and_step_to_points(xmin: float, xmax: float, dx: float) -&gt; int:\n    \"\"\"Calculates the number of points from the\n    endpoints and the stepsize\n\n    Eq:\n        Nx = 1 + floor((xmax-xmin)) / dx)\n\n    Args:\n        xmin (Array | float): the input start point\n        xmax (Array | float): the input end point\n        dx (Array | float): stepsize between each point.\n\n    Returns:\n        Nx (Array | int): the number of points\n    \"\"\"\n    Lx = abs(float(xmax) - float(xmin))\n    return int(floor(1.0 + float(Lx) / float(dx)))\n</code></pre>"}]}